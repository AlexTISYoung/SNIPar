{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<module 'ldsc_72' from 'c:\\\\Users\\\\Hariharan\\\\Documents\\\\git_repos\\\\SNIPar\\\\ldsc_reg\\\\ldsc_72.py'>"
     },
     "metadata": {},
     "execution_count": 80
    }
   ],
   "source": [
    "import importlib\n",
    "import ldsc_71 as ld1\n",
    "import ldsc_72 as ld2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from numba import jit, njit, prange, vectorize\n",
    "from helperfuncs import *\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# reloading modules\n",
    "importlib.reload(ld1)\n",
    "importlib.reload(ld2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Warning there is no value for theta. Maybe consider simulating it\nNo value for U given. Generating a vector of ones (all SNPs weighted equally)\nNo value for r given. Generating a vector of ones for r\nWarning: No value given for allele frequencies. Some parameters won't be noramlized.\nEffect Vectors Simulated!\n"
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "N = 100\n",
    "S = np.array([np.array([[5, 0], [0, 5]]),\n",
    "    np.array([[2, 0], [0, 2]])] * 50 )# 50 = N/2\n",
    "V = np.identity(2) * 10.0\n",
    "\n",
    "\n",
    "model = ld2.sibreg(S = S)\n",
    "model.simdata(V, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "No initial guess provided.\nMaking 'optimal' matrix\n=================================================\n[[ 9.8463384   0.78919482]\n [ 0.78919482 13.19298455]]\n      fun: 552.7864348018542\n hess_inv: <3x3 LbfgsInvHessProduct with dtype=float64>\n      jac: array([ 2.11019484e-06, -7.31265934e-06,  2.33221220e-06])\n  message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'\n     nfev: 8\n      nit: 6\n   status: 0\n  success: True\n        x: array([ 984.63384033,   78.91948151, 1319.2984548 ])\n"
    }
   ],
   "source": [
    "output_matrix, result = model.solve()\n",
    "print(output_matrix)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "jkse = model.jackknife_se()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "No value for U given. Generating a vector of ones (all SNPs weighted equally)\nNo value for r given. Generating a vector of ones for r\n"
    }
   ],
   "source": [
    "# reading in causal data\n",
    "file = \"C:/Users/Hariharan/Documents/genoecon_work/snipardata/causal.hdf5\"\n",
    "\n",
    "hf = h5py.File(file, 'r')\n",
    "theta  = hf.get('estimate')[()]\n",
    "S = hf.get('estimate_covariance')[()]\n",
    "f = hf.get('freqs')[()]\n",
    "\n",
    "model = ld2.sibreg(S = S, f= f, theta = theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "No initial guess provided.\nMaking 'optimal' matrix\n=================================================\nLog Likelihood:  76632.03730018022\nGradient:  [-10186540.70423764   -508426.61071043   -997109.1144656\n  -4332468.92507793  -3209491.84670402  -5596069.28353835]\nLog Likelihood:  77412.07761113478\nGradient:  [-10416331.27452042  -1026599.35153097  -1640989.33435947\n  -5006790.66128204  -4049718.66740726  -6520170.78201466]\nLog Likelihood:  76762.9939863377\nGradient:  [-10219230.42849685   -591790.91854022  -1101879.86469774\n  -4439620.17149002  -3344581.84149721  -5742576.37461988]\nLog Likelihood:  76658.62022211145\nGradient:  [-10192969.54724091   -525237.13776158  -1018282.41066966\n  -4354028.29734585  -3236733.46447701  -5625534.09391698]\nLog Likelihood:  76637.60423465294\nGradient:  [-10187878.17565956   -511942.38727446  -1001539.3344897\n  -4336975.82952856  -3215189.24522277  -5602228.21549751]\nLog Likelihood:  76633.21048499933\nGradient:  [-10186822.17445776   -509167.32706766   -998042.57808347\n  -4333418.36505682  -3210692.19738586  -5597366.71983334]\nLog Likelihood:  76632.28486529221\nGradient:  [-10186600.08262176   -508582.90738757   -997306.08621669\n  -4332669.25995945  -3209745.12976255  -5596343.04562481]\nLog Likelihood:  76632.08955581348\nGradient:  [-10186553.23695436   -508459.60115002   -997150.69060386\n  -4332511.21073508  -3209545.30862249  -5596127.067782  ]\nLog Likelihood:  76632.04833085946\nGradient:  [-10186543.34974327   -508433.574668     -997117.89078764\n  -4332477.85115657  -3209503.1319898   -5596081.4812111 ]\nLog Likelihood:  76632.03962868347\nGradient:  [-10186541.26268464   -508428.08075461   -997110.96708798\n  -4332470.80931221  -3209494.22895127  -5596071.85838372]\nLog Likelihood:  76632.03779171273\nGradient:  [-10186540.82212229   -508426.92102772   -997109.50554282\n  -4332469.32282818  -3209492.34958177  -5596069.82707237]\nLog Likelihood:  76632.03740394016\nGradient:  [-10186540.72912238   -508426.67621654   -997109.19701965\n  -4332469.00904062  -3209491.9528585   -5596069.39827512]\nLog Likelihood:  76632.03732208369\nGradient:  [-10186540.70949066   -508426.62453838   -997109.13189226\n  -4332468.94280195  -3209491.8691126   -5596069.30775862]\nLog Likelihood:  76632.03730480367\nGradient:  [-10186540.70534652   -508426.61362942   -997109.11814426\n  -4332468.92881938  -3209491.8514343   -5596069.28865112]\nLog Likelihood:  76632.03730115622\nGradient:  [-10186540.70447166   -508426.61132661   -997109.11524214\n  -4332468.92586773  -3209491.84770253  -5596069.28461765]\nLog Likelihood:  76632.03730038613\nGradient:  [-10186540.70428707   -508426.61084049   -997109.11462952\n  -4332468.92524466  -3209491.84691479  -5596069.2837662 ]\nLog Likelihood:  76632.037300224\nGradient:  [-10186540.70424804   -508426.61073788   -997109.1145002\n  -4332468.92511313  -3209491.84674849  -5596069.28358644]\nLog Likelihood:  76632.03730018964\nGradient:  [-10186540.70423981   -508426.61071622   -997109.11447289\n  -4332468.92508538  -3209491.84671341  -5596069.2835485 ]\nLog Likelihood:  76632.0373001823\nGradient:  [-10186540.70423809   -508426.61071165   -997109.11446713\n  -4332468.92507949  -3209491.84670598  -5596069.28354051]\nLog Likelihood:  76632.03730018072\nGradient:  [-10186540.70423772   -508426.61071068   -997109.11446592\n  -4332468.92507826  -3209491.84670442  -5596069.28353883]\nLog Likelihood:  76632.03730018032\nGradient:  [-10186540.70423765   -508426.61071048   -997109.11446566\n  -4332468.92507799  -3209491.8467041   -5596069.28353845]\nFinal Estimate:\n [[ 4.06681463  0.15601044 -0.81027527]\n [ 0.15601044 15.64789364 -9.177272  ]\n [-0.81027527 -9.177272   12.385615  ]]\nConvergence Flag:  b'ABNORMAL_TERMINATION_IN_LNSRCH'\nNumber of Iterations:  0\nFinal Gradient:  [10186540.70423764   508426.61071043   997109.1144656   4332468.92507793\n  3209491.84670402  5596069.28353835]\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(array([[ 4.06681463,  0.15601044, -0.81027527],\n        [ 0.15601044, 15.64789364, -9.177272  ],\n        [-0.81027527, -9.177272  , 12.385615  ]]),\n (array([ 4.06681463,  0.15601044, -0.81027527, 15.64789364, -9.177272  ,\n         12.385615  ]),\n  -76632.03730018032,\n  {'grad': array([10186540.70423764,   508426.61071043,   997109.1144656 ,\n           4332468.92507793,  3209491.84670402,  5596069.28353835]),\n   'task': b'ABNORMAL_TERMINATION_IN_LNSRCH',\n   'funcalls': 21,\n   'nit': 0,\n   'warnflag': 2}))"
     },
     "metadata": {},
     "execution_count": 125
    }
   ],
   "source": [
    "model.solve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# restricting to only direct effects\n",
    "theta_dir = theta[:, 0]\n",
    "S_dir = S[:, 0, 0]\n",
    "snp = np.arange(1, len(theta_dir) + 1, 1)\n",
    "\n",
    "simulated_data_out = pd.DataFrame({'b' : theta_dir,\n",
    "                                    'se' : S_dir,\n",
    "                                    'snp' : snp})\n",
    "\n",
    "simulated_data_out.to_csv(\"simulated_data_dir.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([ 59.2012056 ,  53.11456529,  42.85647157, ...,   6.75836736,\n       -78.36891209, -10.75368849])"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "zval = (theta_dir - 0)/S_dir\n",
    "zval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def core_logll_loop(V, N, S, theta, u, r):\n",
    "    \n",
    "    Gvec = np.zeros_like(V)\n",
    "    log_ll = 0\n",
    "        \n",
    "    for i in prange(N):\n",
    "\n",
    "\n",
    "        Si = S[i]\n",
    "        thetai = theta[i, :]\n",
    "        ui = u[i]\n",
    "        ri = r[i]\n",
    "        \n",
    "\n",
    "        d, ddash = Si.shape\n",
    "        assert d == ddash # Each S has to be a square matrix\n",
    "\n",
    "        # calculate log likelihood\n",
    "        log_ll += -(d/2) * np.log(2 * np.pi)\n",
    "        dit_sv = np.linalg.det(Si + ri * V)\n",
    "        log_ll += -(1/2) * np.log(dit_sv)\n",
    "        log_ll += -(1/2) * np.trace(np.outer(thetai, thetai) @ np.linalg.inv(Si + ri * V))\n",
    "        log_ll *= 1/ui\n",
    "\n",
    "        # calculate gradient\n",
    "        SV_inv = np.linalg.inv(Si + ri * V)\n",
    "        G = -(1 / 2) * SV_inv\n",
    "        G += (1 / 2) * np.dot(SV_inv,np.dot(np.outer(thetai, thetai),SV_inv))\n",
    "        G *= 1/ui\n",
    "\n",
    "        Gvec += G\n",
    "        \n",
    "    return log_ll, Gvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def numba_core_logll_loop(V, N, S, theta, u, r):\n",
    "    \n",
    "    Gvec = np.zeros_like(V)\n",
    "    log_ll = 0\n",
    "        \n",
    "    for i in prange(N):\n",
    "\n",
    "\n",
    "        Si = S[i]\n",
    "        thetai = theta[i, :]\n",
    "        ui = u[i]\n",
    "        ri = r[i]\n",
    "        \n",
    "\n",
    "        d, ddash = Si.shape\n",
    "        assert d == ddash # Each S has to be a square matrix\n",
    "\n",
    "        # calculate log likelihood\n",
    "        log_ll += -(d/2) * np.log(2 * np.pi)\n",
    "        dit_sv = np.linalg.det(Si + ri * V)\n",
    "        log_ll += -(1/2) * np.log(dit_sv)\n",
    "        log_ll += -(1/2) * np.trace(np.outer(thetai, thetai) @ np.linalg.inv(Si + ri * V))\n",
    "        log_ll *= 1/ui\n",
    "\n",
    "        # calculate gradient\n",
    "        SV_inv = np.linalg.inv(Si + ri * V)\n",
    "        G = -(1 / 2) * SV_inv\n",
    "        G += (1 / 2) * np.dot(SV_inv,np.dot(np.outer(thetai, thetai),SV_inv))\n",
    "        G *= 1/ui\n",
    "\n",
    "        Gvec += G\n",
    "        \n",
    "    return log_ll, Gvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outer_neg_logll_grad(V, theta, S, u, r, loopfunc):\n",
    "        \n",
    "    # ============================================ #\n",
    "    # returns negative log likelihood and negative\n",
    "    # of the gradient\n",
    "    # ============================================ #\n",
    "    \n",
    "    # Unflatten V into a matrix\n",
    "    d = S[0].shape[0]\n",
    "    V = return_to_symmetric(V, d)\n",
    "    \n",
    "    N = len(S)\n",
    "\n",
    "    log_ll, Gvec = loopfunc(V, N, S, theta, u, r)\n",
    "\n",
    "\n",
    "    Gvec = extract_upper_triangle(Gvec)\n",
    "\n",
    "    return -log_ll, -Gvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vin = extract_upper_triangle(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.1 ms ± 188 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "outer_neg_logll_grad(Vin, model.theta, model.S, model.u, model.r, \n",
    "                     loopfunc = core_logll_loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "801 µs ± 35.4 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "outer_neg_logll_grad(Vin, model.theta, model.S, model.u, model.r,\n",
    "                     loopfunc = numba_core_logll_loop)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}