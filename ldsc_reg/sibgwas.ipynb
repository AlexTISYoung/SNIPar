{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "from scipy.special import factorial\n",
    "from scipy.optimize import minimize, fmin_l_bfgs_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is conducting some simulations from section 7 of ```sib_gwas.pdf```\n",
    "\n",
    "# The Data Generating Process\n",
    "\n",
    "$$\n",
    "\\hat{\\theta}_i | \\theta_i \\sim \\mathcal{N}(\\theta_i, S_i)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\theta_i \\sim \\mathcal{N}(O, V)\n",
    "$$\n",
    "\n",
    "We want to infer $V$.\n",
    "\n",
    "# The likelihood function\n",
    "\n",
    "The contribution of SNP $i$ to the log likelihood is:\n",
    "\n",
    "$$\n",
    "l_i = -\\frac{d}{2} log(2 \\pi) - \\frac{1}{2} log |S_i + V| - \\frac{1}{2}tr(\\hat{\\theta}_i \\hat{\\theta}_i^T(S_i + V)^{-1})\n",
    "$$\n",
    "\n",
    "The gradient:\n",
    "\n",
    "$$\n",
    "\\frac{dl_i}{dV} = -\\frac{1}{2}(S_i + V)^{-1} + \\frac{1}{2} (S_i + V)^{-1} \\hat{\\theta}_i \\hat{\\theta}_i^T (S_i + V)^{-1}\n",
    "$$\n",
    "\n",
    "For now we simply sum across all SNPs to get the log-likelihood and the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first few functions are to allow us to switch between a \"flat\" array into a matrix and back. This is so that we can make parameter restrictions more easily. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_upper_triangle(x):\n",
    "    \n",
    "    # Extracts the upper \n",
    "    \n",
    "    n, m = x.shape\n",
    "    assert n == m\n",
    "    \n",
    "    upper_triangle = x[np.triu_indices(n)]\n",
    "    \n",
    "    return upper_triangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_to_symmetric(triangle_vec, final_size):\n",
    "    \n",
    "    # Given a vector of the upper triangular matrix,\n",
    "    # get back the symmetric matrix\n",
    "    \n",
    "    X = np.zeros((final_size,final_size))\n",
    "    X[np.triu_indices(X.shape[0], k = 0)] = triangle_vec\n",
    "    X = X + X.T - np.diag(np.diag(X))\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```simdata``` simulates a vector of $\\theta$ given $V, S$, and $N$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simdata(V, S, N):\n",
    "    \n",
    "    # Simulated data (theta hats) as per section 7.1\n",
    "    # V = varcov matrix of true effects\n",
    "    # S = array of variance covariance matrices (each one\n",
    "    # for a given snp)\n",
    "    # N = Number of obs/SNPs to generate\n",
    "    # Make sure S has as man\n",
    "    \n",
    "    θhat_vec = []\n",
    "    \n",
    "    # make sure they are np arrays\n",
    "    for i in range(N):\n",
    "        \n",
    "        Si = S[i]\n",
    "        \n",
    "        V = np.array(V)\n",
    "        Si = np.array(Si)\n",
    "\n",
    "        # get shape of V\n",
    "        d = V.shape[0]\n",
    "        zeromat = np.zeros(d)\n",
    "\n",
    "        # generate true effect vector\n",
    "        θ = np.random.multivariate_normal(zeromat, V)\n",
    "\n",
    "        sim = np.random.multivariate_normal(θ, Si)\n",
    "        \n",
    "        # Append to vector of effects\n",
    "        θhat_vec.append(sim)\n",
    "    \n",
    "    θhat_vec = np.array(θhat_vec)\n",
    "    return θhat_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "S = np.array([np.array([[5, 0], [0, 5]]),\n",
    "    np.array([[2, 0], [0, 2]])] * 50 )# 50 = N/2\n",
    "V = np.identity(2) * 10.0\n",
    "\n",
    "\n",
    "θhat_vec = simdata(V, S, N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log likelihood function actually returns the negative log likelihood so that we can minimize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logll(V, θ, S):\n",
    "    \n",
    "    # calculate negative log likelihood\n",
    "    \n",
    "    # Unflatten V into a matrix\n",
    "    d = S[0].shape[0]\n",
    "    V = return_to_symmetric(V, d)\n",
    "    N = len(S)\n",
    "    log_ll = 0\n",
    "    \n",
    "    for i in range(N):\n",
    "           \n",
    "        Si = S[i]\n",
    "        θi = θ[i, :]\n",
    "        d, ddash = Si.shape\n",
    "        assert d == ddash # Each S has to be a square matrix\n",
    "        \n",
    "        log_ll += -(d/2) * np.log(2 * np.pi)\n",
    "        log_ll += -(1/2) * np.log(np.linalg.det(Si + V))\n",
    "        log_ll += -(1/2) * np.trace(np.outer(θi, θi) @ np.linalg.inv(Si + V))\n",
    "        \n",
    "    return -log_ll    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "536.1436769422421"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vvec = np.array([10., 0, 10.])\n",
    "logll(Vvec, θhat_vec, S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_logll(V, θ, S):\n",
    "    # the gradient of the log\n",
    "    # likelihood function\n",
    "    # Unflatten V into a matrix\n",
    "    d = S[0].shape[0]\n",
    "    V = return_to_symmetric(V, d)\n",
    "    N = len(S)\n",
    "    Gvec = np.zeros((d, d))\n",
    "    for i in range(N):\n",
    "        Si = S[i]\n",
    "        θi = θ[i, :]\n",
    "        SV_inv = np.linalg.inv(Si + V)\n",
    "        G = -(1 / 2) * SV_inv\n",
    "        G += (1 / 2) * np.dot(SV_inv,np.dot(np.outer(θi, θi),SV_inv))\n",
    "        Gvec += G\n",
    "\n",
    "    Gvec = extract_upper_triangle(Gvec)\n",
    "    return -Gvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.84555143,  0.64202002, -0.21064045])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_logll(Vvec, θhat_vec, S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```neg_logll_grad``` is an additional function which calucaltes the log likelihood and the gradient together. This is so that we can use ```fmin_l_bfgs_b``` much better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_logll_grad(V, θ, S):\n",
    "    \n",
    "    # returns negative log likelihood and negative\n",
    "    # of the gradient\n",
    "  \n",
    "    # Unflatten V into a matrix\n",
    "    d = S[0].shape[0]\n",
    "    V = return_to_symmetric(V, d)\n",
    "    Gvec = np.zeros((d, d))\n",
    "    \n",
    "    N = len(S)\n",
    "    log_ll = 0\n",
    "    \n",
    "    for i in range(N):\n",
    "        \n",
    "    \n",
    "        Si = S[i]\n",
    "        θi = θ[i, :]\n",
    "        d, ddash = Si.shape\n",
    "        assert d == ddash # Each S has to be a square matrix\n",
    "  \n",
    "        # calculate log likelihood\n",
    "        log_ll += -(d/2) * np.log(2 * np.pi)\n",
    "        log_ll += -(1/2) * np.log(np.linalg.det(Si + V))\n",
    "        log_ll += -(1/2) * np.trace(np.outer(θi, θi) @ np.linalg.inv(Si + V))\n",
    "        \n",
    "        \n",
    "        # calculate gradient\n",
    "        SV_inv = np.linalg.inv(Si + V)\n",
    "        G = -(1 / 2) * SV_inv\n",
    "        G += (1 / 2) * np.dot(SV_inv,np.dot(np.outer(θi, θi),SV_inv))\n",
    "        \n",
    "        Gvec += G\n",
    "\n",
    "    Gvec = extract_upper_triangle(Gvec)\n",
    "    \n",
    "    return -log_ll, -Gvec    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(536.1436769422421, array([ 0.84555143,  0.64202002, -0.21064045]))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_logll_grad(Vvec, θhat_vec, S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets do some optimizing!\n",
    "\n",
    "def solve(negloglik, est_init, \n",
    "          negloglike_args, bounds):\n",
    "    \n",
    "    # == Solves our MLE problem == #\n",
    "    \n",
    "    # convert matrix to a single dimension vector\n",
    "    est_init_array = extract_upper_triangle(est_init)\n",
    "    \n",
    "    # \n",
    "\n",
    "    result = fmin_l_bfgs_b(\n",
    "        negloglik, \n",
    "        est_init_array,\n",
    "        fprime = None,\n",
    "        args = negloglike_args,\n",
    "        bounds = bounds\n",
    "    )\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 7.13293055, -2.26934301, 10.82120616]),\n",
       " 532.8231165666621,\n",
       " {'grad': array([-4.47869097e-05, -2.34670997e-05, -4.95931317e-05]),\n",
       "  'task': b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH',\n",
       "  'funcalls': 9,\n",
       "  'nit': 7,\n",
       "  'warnflag': 0})"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solve(neg_logll_grad, np.array([[10., 0.],\n",
    "                                [0., 10.]]),\n",
    "     (θhat_vec, S), [(1e-5, None),\n",
    "                    (None, None),\n",
    "                    (1e-5, None)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
