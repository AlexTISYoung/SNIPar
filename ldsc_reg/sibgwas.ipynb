{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "from scipy.special import factorial\n",
    "from scipy.optimize import minimize, fmin_l_bfgs_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is conducting some simulations from section 7 of ```sib_gwas.pdf```\n",
    "\n",
    "# The Data Generating Process\n",
    "\n",
    "$$\n",
    "\\hat{\\theta}_i | \\theta_i \\sim \\mathcal{N}(\\theta_i, S_i)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\theta_i \\sim \\mathcal{N}(O, V)\n",
    "$$\n",
    "\n",
    "We want to infer $V$.\n",
    "\n",
    "# The likelihood function\n",
    "\n",
    "The contribution of SNP $i$ to the log likelihood is:\n",
    "\n",
    "$$\n",
    "l_i = -\\frac{d}{2} log(2 \\pi) - \\frac{1}{2} log |S_i + V| - \\frac{1}{2}tr(\\hat{\\theta}_i \\hat{\\theta}_i^T(S_i + V)^{-1})\n",
    "$$\n",
    "\n",
    "The gradient:\n",
    "\n",
    "$$\n",
    "\\frac{dl_i}{dV} = -\\frac{1}{2}(S_i + V)^{-1} + \\frac{1}{2} (S_i + V)^{-1} \\hat{\\theta}_i \\hat{\\theta}_i^T (S_i + V)^{-1}\n",
    "$$\n",
    "\n",
    "For now we simply sum across all SNPs to get the log-likelihood and the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first few functions are to allow us to switch between a \"flat\" array into a matrix and back. This is so that we can make parameter restrictions more easily. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_upper_triangle(x):\n",
    "    \n",
    "    # Extracts the upper \n",
    "    \n",
    "    n, m = x.shape\n",
    "    assert n == m\n",
    "    \n",
    "    upper_triangle = x[np.triu_indices(n)]\n",
    "    \n",
    "    return upper_triangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_bounds(n):\n",
    "    \n",
    "    # From a number n, the function\n",
    "    # uutputs a list of bounds\n",
    "    # for a var cov matrix of size\n",
    "    # n x n\n",
    "    \n",
    "    # extract idx of flat array whcih are diagonals\n",
    "    uptriangl_idx = np.array(np.triu_indices(n))\n",
    "    diags = uptriangl_idx[0, :] == uptriangl_idx[1, :]\n",
    "    \n",
    "    # Construct list of bounds\n",
    "    bounds_list = np.array([(None, None)] * len(diags))\n",
    "    bounds_list[diags] = (1e-6, None)\n",
    "    \n",
    "    bounds_list_out = [tuple(i) for i in bounds_list]\n",
    "    \n",
    "    return bounds_list_out\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_to_symmetric(triangle_vec, final_size):\n",
    "    \n",
    "    # Given a vector of the upper triangular matrix,\n",
    "    # get back the symmetric matrix\n",
    "    \n",
    "    X = np.zeros((final_size,final_size))\n",
    "    X[np.triu_indices(X.shape[0], k = 0)] = triangle_vec\n",
    "    X = X + X.T - np.diag(np.diag(X))\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```simdata``` simulates a vector of $\\theta$ given $V, S$, and $N$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simdata(V, S, N):\n",
    "    \n",
    "    # Simulated data (theta hats) as per section 7.1\n",
    "    # V = varcov matrix of true effects\n",
    "    # S = array of variance covariance matrices (each one\n",
    "    # for a given snp)\n",
    "    # N = Number of obs/SNPs to generate\n",
    "    # Make sure S has as man\n",
    "    \n",
    "    θhat_vec = []\n",
    "    \n",
    "    # make sure they are np arrays\n",
    "    for i in range(N):\n",
    "        \n",
    "        Si = S[i]\n",
    "        \n",
    "        V = np.array(V)\n",
    "        Si = np.array(Si)\n",
    "\n",
    "        # get shape of V\n",
    "        d = V.shape[0]\n",
    "        zeromat = np.zeros(d)\n",
    "\n",
    "        # generate true effect vector\n",
    "        θ = np.random.multivariate_normal(zeromat, V)\n",
    "\n",
    "        sim = np.random.multivariate_normal(θ, Si)\n",
    "        \n",
    "        # Append to vector of effects\n",
    "        θhat_vec.append(sim)\n",
    "    \n",
    "    θhat_vec = np.array(θhat_vec)\n",
    "    return θhat_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "S = np.array([np.array([[5, 0], [0, 5]]),\n",
    "    np.array([[2, 0], [0, 2]])] * 50 )# 50 = N/2\n",
    "V = np.identity(2) * 10.0\n",
    "\n",
    "\n",
    "θhat_vec = simdata(V, S, N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log likelihood function actually returns the negative log likelihood so that we can minimize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logll(V, θ, S):\n",
    "    \n",
    "    # calculate negative log likelihood\n",
    "    \n",
    "    # Unflatten V into a matrix\n",
    "    d = S[0].shape[0]\n",
    "    V = return_to_symmetric(V, d)\n",
    "    N = len(S)\n",
    "    log_ll = 0\n",
    "    \n",
    "    for i in range(N):\n",
    "           \n",
    "        Si = S[i]\n",
    "        θi = θ[i, :]\n",
    "        d, ddash = Si.shape\n",
    "        assert d == ddash # Each S has to be a square matrix\n",
    "        \n",
    "        log_ll += -(d/2) * np.log(2 * np.pi)\n",
    "        log_ll += -(1/2) * np.log(np.linalg.det(Si + V))\n",
    "        log_ll += -(1/2) * np.trace(np.outer(θi, θi) @ np.linalg.inv(Si + V))\n",
    "        \n",
    "    return -log_ll    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "554.8919228079757"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vvec = np.array([10., 0, 10.])\n",
    "logll(Vvec, θhat_vec, S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_logll(V, θ, S):\n",
    "    # the gradient of the log\n",
    "    # likelihood function\n",
    "    # Unflatten V into a matrix\n",
    "    d = S[0].shape[0]\n",
    "    V = return_to_symmetric(V, d)\n",
    "    N = len(S)\n",
    "    Gvec = np.zeros((d, d))\n",
    "    for i in range(N):\n",
    "        Si = S[i]\n",
    "        θi = θ[i, :]\n",
    "        SV_inv = np.linalg.inv(Si + V)\n",
    "        G = -(1 / 2) * SV_inv\n",
    "        G += (1 / 2) * np.dot(SV_inv,np.dot(np.outer(θi, θi),SV_inv))\n",
    "        Gvec += G\n",
    "\n",
    "    Gvec = extract_upper_triangle(Gvec)\n",
    "    return -Gvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.42958569,  0.51072982, -0.27180245])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_logll(Vvec, θhat_vec, S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```neg_logll_grad``` is an additional function which calucaltes the log likelihood and the gradient together. This is so that we can use ```fmin_l_bfgs_b``` much better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_logll_grad(V, θ, S):\n",
    "    \n",
    "    # returns negative log likelihood and negative\n",
    "    # of the gradient\n",
    "  \n",
    "    # Unflatten V into a matrix\n",
    "    d = S[0].shape[0]\n",
    "    V = return_to_symmetric(V, d)\n",
    "    Gvec = np.zeros((d, d))\n",
    "    \n",
    "    N = len(S)\n",
    "    log_ll = 0\n",
    "    \n",
    "    for i in range(N):\n",
    "        \n",
    "    \n",
    "        Si = S[i]\n",
    "        θi = θ[i, :]\n",
    "        d, ddash = Si.shape\n",
    "        assert d == ddash # Each S has to be a square matrix\n",
    "  \n",
    "        # calculate log likelihood\n",
    "        log_ll += -(d/2) * np.log(2 * np.pi)\n",
    "        log_ll += -(1/2) * np.log(np.linalg.det(Si + V))\n",
    "        log_ll += -(1/2) * np.trace(np.outer(θi, θi) @ np.linalg.inv(Si + V))\n",
    "        \n",
    "        \n",
    "        # calculate gradient\n",
    "        SV_inv = np.linalg.inv(Si + V)\n",
    "        G = -(1 / 2) * SV_inv\n",
    "        G += (1 / 2) * np.dot(SV_inv,np.dot(np.outer(θi, θi),SV_inv))\n",
    "        \n",
    "        Gvec += G\n",
    "\n",
    "    Gvec = extract_upper_triangle(Gvec)\n",
    "    \n",
    "    return -log_ll, -Gvec    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(554.8919228079757, array([-0.42958569,  0.51072982, -0.27180245]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_logll_grad(Vvec, θhat_vec, S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets do some optimizing!\n",
    "\n",
    "def solve(θ, S,\n",
    "          negloglik, est_init = None):\n",
    "    \n",
    "    # == Solves our MLE problem == #\n",
    "    \n",
    "    n, m = θ.shape\n",
    "    \n",
    "    if est_init is not None:\n",
    "        # Shape of initial varcov guess\n",
    "        rowstrue = est_init.shape[0] == m\n",
    "        colstrue = est_init.shape[1] == m\n",
    "\n",
    "        if rowstrue & colstrue:\n",
    "            pass\n",
    "        else:\n",
    "            print(\"Warning: Initial Estimate given is not of the proper dimension\")\n",
    "            print(\"Making a matrix of 0s as the initial estimate\")\n",
    "            \n",
    "            est_init = np.zeros((m, m))\n",
    "    else:\n",
    "        print(\"No initial guess provided.\")\n",
    "        print(\"Making a matrix of 0s as the initial estimate\")\n",
    "        \n",
    "        est_init = np.zeros((m, m))\n",
    "        \n",
    "        \n",
    "    \n",
    "    # extract array from est init\n",
    "    est_init_array = extract_upper_triangle(est_init) \n",
    "    \n",
    "    bounds = extract_bounds(m)\n",
    "\n",
    "    result = fmin_l_bfgs_b(\n",
    "        negloglik, \n",
    "        est_init_array,\n",
    "        fprime = None,\n",
    "        args = (θ, S),\n",
    "        bounds = bounds\n",
    "    )\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No initial guess provided.\n",
      "Making a matrix of 0s as the initial estimate\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([11.6083141 , -1.90505703, 11.06731912]),\n",
       " 553.62038068952,\n",
       " {'grad': array([3.13997460e-06, 3.55105301e-06, 3.67615960e-06]),\n",
       "  'task': b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL',\n",
       "  'funcalls': 14,\n",
       "  'nit': 13,\n",
       "  'warnflag': 0})"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solve(θhat_vec, S,\n",
    "      neg_logll_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
