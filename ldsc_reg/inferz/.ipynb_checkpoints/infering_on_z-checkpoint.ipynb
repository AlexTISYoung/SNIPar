{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helperfuncs as hp\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import comb\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the PDF and the Log Likelihoods\n",
    "\n",
    "Consider a random variable $z_i$ of length $d$ given by:\n",
    "\n",
    "$$\n",
    "z_i \\sim \\mathcal{N}(0, I + r_i S_i^{-1/2} V S_i^{-1/2})\n",
    "$$\n",
    "\n",
    "The log likelihood $l$ for a SNP $i$ is given by:\n",
    "\n",
    "$$\n",
    "l_i = -\\frac{d}{2} log(2 \\pi) - \\frac{1}{2} log |I + r_i S_i^{-1/2} V S_i^{-1/2} | - \\frac{1}{2} z_i^T (I + r_i S_i^{-1/2} V S_i^{-1/2}) ^ {-1}z_i \\tag{1}\n",
    "$$\n",
    "\n",
    "## Solving for the gradient\n",
    "\n",
    "Refs: \n",
    "- https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf\n",
    "- https://blog.as.uky.edu/sta707/wp-content/uploads/2014/01/matrix-derivatives.pdf\n",
    "- https://stats.stackexchange.com/questions/351549/maximum-likelihood-estimators-multivariate-gaussian\n",
    "\n",
    "The first term from equation 1 doesn't have V. So it disappears from the derivative.\n",
    "\n",
    "The second term:\n",
    "\n",
    "$$\n",
    "f_2 = \\frac{1}{2} log |I + r_i S_i^{-1/2} V S_i^{-1/2}| \\\\\n",
    "\\frac{\\partial f_2}{\\partial V_{ij}}  = \\frac{1}{2} Tr((I + r_i S_i^{-1/2} V S_i^{-1/2})^{-1} \\frac{\\partial (I + r_i S_i^{-1/2} V S_i^{-1/2})}{\\partial V_{ij}}) \\\\\n",
    "    = \\frac{1}{2} Tr((I + r_i S_i^{-1/2} V S_i^{-1/2})^{-1}(r_i  S_i^{-1/2} 0^{ij = 1} S_i^{-1/2})^T)\n",
    "$$\n",
    "\n",
    "\n",
    "Where $0^{ij = 1}$ is a 0 matrix with the same dimensions as $V$ and the $ij$ position as 1. Let the vector of $\\frac{\\partial f_2}{\\partial V_{ij}}$ be $F_2$.\n",
    "\n",
    "The third term:\n",
    "\n",
    "$$\n",
    "f_3 = \\frac{1}{2} z_i^T (I + r_i S_i^{-1/2} V S_i^{-1/2}) ^ {-1}z_i \\\\\n",
    "    = \\frac{1}{2} tr(z_i z_i^T (I + r_i S_i^{-1/2} V S_i^{-1/2}) ^ {-1}) \\\\\n",
    "\\frac{\\partial f_3}{\\partial V} = z_i z_i^T [-(I + r_i S_i^{-1/2} V S_i^{-1/2}) ^ {-1} (r_i S_i^{-1/2} S_i^{-1/2}) (I + r_i S_i^{-1/2} V S_i^{-1/2}) ^ {-1}] \\\\\n",
    "$$\n",
    "\n",
    "Putting both of these together we get the gradient:\n",
    "\n",
    "$$\n",
    "\\frac{dl_i}{V} =-F_2 + z_i z_i^T [-(I + r_i S_i^{-1/2} V S_i^{-1/2} )^{-1} r_i S_i^{-1/2} S_i^{-1/2} (I + r_i S_i^{-1/2} V S_i^{-1/2})^{-1}]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sibreg():\n",
    "    \n",
    "    def __init__(self, S, z = None, u = None, r = None, f = None):\n",
    "        \n",
    "        if S.ndim > 1:\n",
    "            for s in S:\n",
    "                n, m = s.shape\n",
    "                assert n == m\n",
    "\n",
    "        if z is None:\n",
    "            print(\"Warning there is no value for z. Maybe consider simulating it\")\n",
    "        if u is None:\n",
    "            print(\"No value for U given. Generating a vector of ones (all SNPs weighted equally)\")\n",
    "            u = np.ones(S.shape[0])\n",
    "        if r is None:\n",
    "            print(\"No value for r given. Generating a vector of ones for r\")\n",
    "            r = np.ones(S.shape[0])\n",
    "        if f is None:\n",
    "            print(\"Warning: No value given for allele frequencies. Some parameters won't be noramlized.\")\n",
    "        \n",
    "        self.z = None if z is None else z[~np.any(np.isnan(z), axis = 1)]\n",
    "        self.S = S[~np.any(np.isnan(S), axis = (1, 2))]\n",
    "        self.u = u[~np.isnan(u)]\n",
    "        self.r = r[~np.isnan(r)]\n",
    "        self.f = None if f is None else f[~np.isnan(f)]\n",
    "    \n",
    "\n",
    "    def simdata(self, V,  N, simr = False):\n",
    "        \n",
    "        # Simulated data (theta hats) as per section 7.1\n",
    "        # V = varcov matrix of true effects\n",
    "        # N = Number of obs/SNPs to generate\n",
    "        \n",
    "        S = self.S\n",
    "        \n",
    "        if simr:\n",
    "            self.r = np.random.uniform(low=1, high=5, size=N)\n",
    "            print(\"Simulated LD scores!\")\n",
    "        \n",
    "        r = self.r\n",
    "\n",
    "        zhat_vec = np.empty((N, V.shape[1]))\n",
    "        for i in range(N):\n",
    "            \n",
    "            Si = S[i]\n",
    "            ri = r[i]\n",
    "            \n",
    "            V = np.array(V)\n",
    "            Si = np.array(Si)\n",
    "            S_inv = np.linalg.inv(np.sqrt(Si))\n",
    "  \n",
    "            # get shape of V\n",
    "            d = V.shape[0]\n",
    "            zeromat = np.zeros(d)\n",
    "\n",
    "            # generate true effect vector\n",
    "            if d > 1:\n",
    "                sim = np.random.multivariate_normal(zeromat, np.eye(d) + ri * S_inv @ V @ S_inv)\n",
    "            else:\n",
    "                sim = np.random.normal(zeromat, np.eye(d) + ri * S_inv @ V @ S_inv)\n",
    "            \n",
    "            # Append to vector of effects\n",
    "            zhat_vec[i, :] = sim\n",
    "        \n",
    "\n",
    "        print(\"Effect Vectors Simulated!\")\n",
    "        \n",
    "        self.snp = np.arange(1, N+1, 1)\n",
    "        self.pos = np.arange(1, N+1, 1)\n",
    "        self.z = zhat_vec\n",
    "\n",
    "    def neg_logll_grad(self, V, z = None, S = None, u = None, r = None, f = None):\n",
    "        \n",
    "        # ============================================ #\n",
    "        # returns negative log likelihood and negative\n",
    "        # of the gradient\n",
    "        # ============================================ #\n",
    "        \n",
    "        z = self.z if z is None else z\n",
    "        S = self.S if S is None else S\n",
    "        u = self.u if u is None else u\n",
    "        r = self.r if r is None else r\n",
    "        f = self.f if f is None else f\n",
    "\n",
    "        # Unflatten V into a matrix\n",
    "        d = S[0].shape[0]\n",
    "        V = hp.return_to_symmetric(V, d)\n",
    "        Gvec = np.zeros((d, d))\n",
    "        \n",
    "        N = len(S)\n",
    "        log_ll = 0\n",
    "        \n",
    "        # Normalizing variables\n",
    "        V_norm = V/N\n",
    "        for i in range(N):\n",
    "            \n",
    "            Si = S[i]\n",
    "            zi = z[i, :].reshape((d, 1))\n",
    "            ui = u[i]\n",
    "            ri = r[i]\n",
    "            \n",
    "            \n",
    "            fi = f[i]  if f is not None else None\n",
    "\n",
    "            d, ddash = Si.shape\n",
    "            assert d == ddash # Each S has to be a square matrix\n",
    "            \n",
    "            # normalizing variables using allele frequency\n",
    "#             normalizer = 2 * fi  * (1 - fi) if fi is not None else 1.0\n",
    "#             z = np.sqrt(normalizer) * thetai\n",
    "#             Si = normalizer * Si\n",
    "            \n",
    "            # useful objects\n",
    "            Si_inv = np.linalg.inv(np.sqrt(Si))\n",
    "            sigma_exp = np.eye(d) + ri * Si_inv @ V_norm @ Si_inv\n",
    "            sigma_diff = ri * Si_inv @ Si_inv\n",
    "      \n",
    "            # calculate log likelihood\n",
    "            log_ll_add = -(d/2) * np.log(2 * np.pi)\n",
    "            dit_sv = np.linalg.det(sigma_exp)\n",
    "            dit_sv = 1e-6 if dit_sv < 0 else dit_sv\n",
    "            log_ll_add += -(1/2) * np.log(dit_sv)\n",
    "            log_ll_add += -(1/2) * np.trace(zi @ zi.T @ np.linalg.inv(sigma_exp))\n",
    "            log_ll_add *= 1/ui\n",
    "            \n",
    "            if np.isnan(log_ll_add) == False:\n",
    "                log_ll += log_ll_add\n",
    "            \n",
    "            # calculate gradient\n",
    "            \n",
    "            F2 = np.empty_like(V)\n",
    "            for i in range(V.shape[0]):\n",
    "                for j in range(V.shape[1]):\n",
    "                    zeromat = np.zeros_like(V)\n",
    "                    zeromat[i, j] = 1.0\n",
    "                    F2[i, j] =  (1/2) * np.trace(np.linalg.inv(sigma_exp) @ (ri * Si_inv @ zeromat @ Si_inv)).T\n",
    "                    \n",
    "            \n",
    "            G = -F2\n",
    "            G += zi @ zi.T @ (-np.linalg.inv(sigma_exp) \\\n",
    "                                 @ (sigma_diff) \\\n",
    "                                 @ np.linalg.inv(sigma_exp))\n",
    "            if np.any(np.isnan(G)) == False:\n",
    "                Gvec += G\n",
    "                \n",
    "        print(\"V: \", V)\n",
    "\n",
    "        Gvec = hp.extract_upper_triangle(Gvec)\n",
    "        return -log_ll , -Gvec\n",
    "\n",
    "\n",
    "    def solve(self,\n",
    "              z = None, \n",
    "              S = None,\n",
    "              u = None,\n",
    "              r = None,\n",
    "              f = None,\n",
    "              neg_logll_grad = None,\n",
    "              est_init = None,\n",
    "              printout = True):\n",
    "        \n",
    "        # inherit parameters from the class if they aren't defined\n",
    "        z = self.z if (z is None) else z\n",
    "        S = self.S if S is None else S\n",
    "        u = self.u if u is None else u\n",
    "        r = self.r if r is None else r\n",
    "        f = self.f if f is None else f\n",
    "        neg_logll_grad = self.neg_logll_grad if neg_logll_grad is None else neg_logll_grad\n",
    "\n",
    "        # == Solves our MLE problem == #\n",
    "        n, m = z.shape\n",
    "        \n",
    "        if est_init is not None:\n",
    "            # Shape of initial varcov guess\n",
    "            rowstrue = est_init.shape[0] == m\n",
    "            colstrue = est_init.shape[1] == m\n",
    "\n",
    "            if rowstrue & colstrue:\n",
    "                pass\n",
    "            else:\n",
    "                if printout == True:\n",
    "                    print(\"Warning: Initial Estimate given is not of the proper dimension\")\n",
    "                    print(\"Making 'optimal' matrix\")\n",
    "                    print(\"=================================================\")\n",
    "                \n",
    "                est_init = np.zeros((m, m))\n",
    "        else:\n",
    "            if printout == True:\n",
    "                print(\"No initial guess provided.\")\n",
    "                print(\"Making 'optimal' matrix\")\n",
    "                print(\"=================================================\")\n",
    "            \n",
    "            est_init = np.zeros((m, m))\n",
    "            \n",
    "        \n",
    "        # exporting for potential later reference\n",
    "        self.est_init = est_init\n",
    "\n",
    "        # extract array from est init\n",
    "        est_init_array = hp.extract_upper_triangle(est_init) \n",
    "        \n",
    "        bounds = hp.extract_bounds(m)\n",
    "\n",
    "        result = minimize(\n",
    "            neg_logll_grad, \n",
    "            est_init_array,\n",
    "            jac = True,\n",
    "            args = (z, S, u, r, f),\n",
    "            bounds = bounds,\n",
    "            method = 'L-BFGS-B'\n",
    "        )\n",
    "        \n",
    "        output_matrix = hp.return_to_symmetric(result.x, m)\n",
    "        \n",
    "        # re-normnalizing output matrix\n",
    "        output_matrix = output_matrix / n\n",
    "        \n",
    "        self.output_matrix = output_matrix\n",
    "        \n",
    "        return output_matrix, result \n",
    "\n",
    "    def jackknife_se(self,\n",
    "                  theta  = None, S = None,\n",
    "                  r = None, u = None,\n",
    "                  blocksize = 1):\n",
    "\n",
    "        # Simple jackknife estimator for SE\n",
    "        # Ref: https://github.com/bulik/ldsc/blob/aa33296abac9569a6422ee6ba7eb4b902422cc74/ldscore/jackknife.py#L231\n",
    "        # Default value of blocksize = 1 is the normal jackknife\n",
    "\n",
    "        theta = self.theta if (theta is None) else theta\n",
    "        S = self.S if (S is None) else S\n",
    "        r = self.r if (r is None) else r\n",
    "        u = self.u if (u is None) else u\n",
    "\n",
    "        \n",
    "        assert theta.shape[0] == S.shape[0]\n",
    "\n",
    "        nobs = theta.shape[0]\n",
    "        \n",
    "        estimates_jk = []\n",
    "        \n",
    "        start_idx = 0\n",
    "        while True:\n",
    "            \n",
    "            end_idx = start_idx + blocksize\n",
    "            end_idx_cond = end_idx <= theta.shape[0]\n",
    "            \n",
    "            # remove blocks of observations\n",
    "\n",
    "            vars_jk = []\n",
    "\n",
    "            for var in [theta, S, r, u]:\n",
    "\n",
    "                var_jk = delete_obs_jk(var, start_idx, end_idx,\n",
    "                                       end_idx_cond)\n",
    "                vars_jk.append(var_jk)\n",
    "            \n",
    "            if start_idx < theta.shape[0]:\n",
    "                # Get our estimate\n",
    "                output_matrix, _ = self.solve(theta = vars_jk[0],\n",
    "                                              S = vars_jk[1],\n",
    "                                              r = vars_jk[2],\n",
    "                                              u = vars_jk[3],\n",
    "                                              printout = False,\n",
    "                                              est_init = self.est_init)\n",
    "\n",
    "                estimates_jk.append(output_matrix)\n",
    "\n",
    "                start_idx += blocksize\n",
    "            else:\n",
    "                break\n",
    "            \n",
    "        estimates_jk = np.array(estimates_jk)\n",
    "        full_est = self.output_matrix\n",
    "        \n",
    "        # calculate pseudo-values\n",
    "        n_blocks = int(nobs/blocksize)\n",
    "        pseudovalues = n_blocks * full_est - (n_blocks - 1) * estimates_jk\n",
    "        \n",
    "        # calculate jackknife se\n",
    "        pseudovalues = pseudovalues.reshape((n_blocks, theta.shape[1] * theta.shape[1]))\n",
    "        jknife_cov = np.cov(pseudovalues.T, ddof=1) / n_blocks\n",
    "        jknife_var = np.diag(jknife_cov)\n",
    "        jknife_se = np.sqrt(jknife_var)\n",
    "    \n",
    "        jknife_se  = jknife_se.reshape((theta.shape[1], theta.shape[1]))\n",
    "        \n",
    "        return jknife_se  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N = 100\n",
    "# S_size=  int(N/2)\n",
    "# S = np.array([np.array([[.5, 0], [0, .8]]),\n",
    "#     np.array([[0.5, 0], [0, 0.8]])] * S_size )/N\n",
    "# V = np.identity(2) * 0.5\n",
    "\n",
    "N = 100\n",
    "S_size=  int(N/2)\n",
    "S = np.array([0.5/N] * N).reshape((N, 1, 1))\n",
    "V = np.identity(1) * 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning there is no value for z. Maybe consider simulating it\n",
      "No value for U given. Generating a vector of ones (all SNPs weighted equally)\n",
      "No value for r given. Generating a vector of ones for r\n",
      "Warning: No value given for allele frequencies. Some parameters won't be noramlized.\n",
      "Simulated LD scores!\n",
      "Effect Vectors Simulated!\n"
     ]
    }
   ],
   "source": [
    "model = sibreg(S = S)\n",
    "model.simdata(V, N, simr = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V:  [[50.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(14883.473495079086, array([57933.02988527]))"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vin = hp.extract_upper_triangle(V)\n",
    "model.neg_logll_grad(Vin * N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V:  [[50.]]\n",
      "V:  [[49.]]\n",
      "V:  [[49.79345458]]\n",
      "V:  [[49.95678754]]\n",
      "V:  [[49.99093589]]\n",
      "V:  [[49.99809772]]\n",
      "V:  [[49.99960072]]\n",
      "V:  [[49.99991619]]\n",
      "V:  [[49.99998241]]\n",
      "V:  [[49.99999631]]\n",
      "V:  [[49.99999922]]\n",
      "V:  [[49.99999984]]\n",
      "V:  [[49.99999997]]\n",
      "V:  [[49.99999999]]\n",
      "V:  [[50.]]\n",
      "V:  [[50.]]\n",
      "V:  [[50.]]\n",
      "V:  [[50.]]\n",
      "V:  [[50.]]\n",
      "V:  [[50.]]\n",
      "V:  [[50.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[0.5]]),       fun: 14883.473495079123\n",
       "  hess_inv: <1x1 LbfgsInvHessProduct with dtype=float64>\n",
       "       jac: array([57933.02988527])\n",
       "   message: b'ABNORMAL_TERMINATION_IN_LNSRCH'\n",
       "      nfev: 21\n",
       "       nit: 0\n",
       "    status: 2\n",
       "   success: False\n",
       "         x: array([50.]))"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.solve(est_init = V * N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V:  [[50.000001  0.      ]\n",
      " [ 0.       50.      ]]\n",
      "V:  [[50.  0.]\n",
      " [ 0. 50.]]\n",
      "V:  [[5.e+01 1.e-06]\n",
      " [1.e-06 5.e+01]]\n",
      "V:  [[50.  0.]\n",
      " [ 0. 50.]]\n",
      "V:  [[50.  0.]\n",
      " [ 0. 50.]]\n",
      "V:  [[50.  0.]\n",
      " [ 0. 50.]]\n",
      "V:  [[50.        0.      ]\n",
      " [ 0.       50.000001]]\n",
      "V:  [[50.  0.]\n",
      " [ 0. 50.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.02835873, -0.15541229],\n",
       "       [ 0.        , -0.04679066]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def logll(V):\n",
    "\n",
    "    Vin = hp.extract_upper_triangle(V)\n",
    "    logll = -model.neg_logll_grad(Vin)[0]\n",
    "    \n",
    "    return logll\n",
    "\n",
    "def gradlogll(V):\n",
    "\n",
    "    Vin = hp.extract_upper_triangle(V)\n",
    "    grad = -model.neg_logll_grad(Vin)[1]\n",
    "    \n",
    "    return grad\n",
    "\n",
    "def num_grad(V, row = 0, col = 0, dx = 1e-6):\n",
    "    \n",
    "    V1 = np.copy(V)\n",
    "    V1[row, col] += dx\n",
    "    \n",
    "    derivative = (logll(V1) - logll(V))/dx\n",
    "    \n",
    "    return derivative\n",
    "\n",
    "def jacobian(V, dx = 1e-6):\n",
    "    \n",
    "    rows = V.shape[0]\n",
    "    cols = V.shape[1]\n",
    "    \n",
    "    J = np.empty((rows, cols))\n",
    "    \n",
    "    for row in range(rows):\n",
    "        for col in range(cols):\n",
    "            J[row, col] = num_grad(V, row = row, col = col, dx = dx)\n",
    "    \n",
    "    return J\n",
    "\n",
    "\n",
    "jacobian(V * N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V:  [[50.  0.]\n",
      " [ 0. 50.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-293.15530393,   19.55854529, -288.77055092])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradlogll(V * N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradients dont align!\n",
    "\n",
    "Let's see if the likelihood is correctly specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5076.381911198459"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Si = S[0]\n",
    "ri = model.r[0]\n",
    "S_inv = np.linalg.inv(np.sqrt(Si))\n",
    "logll = scipy.stats.multivariate_normal.logpdf(model.z, \n",
    "                                       mean = np.zeros(2),\n",
    "                                      cov = np.eye(2) + ri * S_inv @ V/N @ S_inv)\n",
    "logll.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V:  [[0.5 0. ]\n",
      " [0.  0.5]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-6846.900914162205"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vin = hp.extract_upper_triangle(V)\n",
    "-model.neg_logll_grad(Vin)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
