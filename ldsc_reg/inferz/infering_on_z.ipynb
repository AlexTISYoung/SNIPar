{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helperfuncs as hp\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import comb\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the PDF and the Log Likelihoods\n",
    "\n",
    "Consider a random variable $z_i$ of length $d$ given by:\n",
    "\n",
    "$$\n",
    "z_i \\sim \\mathcal{N}(0, I + r_i S_i^{-1/2} V S_i^{-1/2})\n",
    "$$\n",
    "\n",
    "The log likelihood $l$ for a SNP $i$ is given by:\n",
    "\n",
    "$$\n",
    "l_i = -\\frac{d}{2} log(2 \\pi) - \\frac{1}{2} log |I + r_i S_i^{-1/2} V S_i^{-1/2} | - \\frac{1}{2} z_i^T (I + r_i S_i^{-1/2} V S_i^{-1/2}) ^ {-1}z_i\n",
    "$$\n",
    "\n",
    "Its derivative with respect to V is given by:\n",
    "\n",
    "$$\n",
    "\\frac{dl_i}{V} =-\\frac{1}{2}\\left[(I + r_i S_i^{-1/2} V _i^{-1/2} S_i^{-1/2}) r_i S_i^{-1/2}S_i^{-1/2} + z_i z_i^T [-(I + r_i S_i^{-1/2} V S_i^{-1/2} ) r_i S_i^{-1/2} S_i^{-1/2} (I + r_i S_i^{-1/2} V S_i^{-1/2})]\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Still to develop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sibreg():\n",
    "    \n",
    "    def __init__(self, S, z = None, u = None, r = None, f = None):\n",
    "        \n",
    "        if S.ndim > 1:\n",
    "            for s in S:\n",
    "                n, m = s.shape\n",
    "                assert n == m\n",
    "\n",
    "        if z is None:\n",
    "            print(\"Warning there is no value for z. Maybe consider simulating it\")\n",
    "        if u is None:\n",
    "            print(\"No value for U given. Generating a vector of ones (all SNPs weighted equally)\")\n",
    "            u = np.ones(S.shape[0])\n",
    "        if r is None:\n",
    "            print(\"No value for r given. Generating a vector of ones for r\")\n",
    "            r = np.ones(S.shape[0])\n",
    "        if f is None:\n",
    "            print(\"Warning: No value given for allele frequencies. Some parameters won't be noramlized.\")\n",
    "        \n",
    "        self.z = None if z is None else z[~np.any(np.isnan(z), axis = 1)]\n",
    "        self.S = S[~np.any(np.isnan(S), axis = (1, 2))]\n",
    "        self.u = u[~np.isnan(u)]\n",
    "        self.r = r[~np.isnan(r)]\n",
    "        self.f = None if f is None else f[~np.isnan(f)]\n",
    "    \n",
    "\n",
    "    def simdata(self, V,  N, simr = False):\n",
    "        \n",
    "        # Simulated data (theta hats) as per section 7.1\n",
    "        # V = varcov matrix of true effects\n",
    "        # N = Number of obs/SNPs to generate\n",
    "        \n",
    "        S = self.S\n",
    "        \n",
    "        if simr:\n",
    "            self.r = np.random.uniform(low=1, high=5, size=N)\n",
    "            print(\"Simulated LD scores!\")\n",
    "        \n",
    "        r = self.r\n",
    "\n",
    "        zhat_vec = np.empty((N, V.shape[1]))\n",
    "        for i in range(N):\n",
    "            \n",
    "            Si = S[i]\n",
    "            ri = r[i]\n",
    "            \n",
    "            V = np.array(V)/N\n",
    "            Si = np.array(Si)\n",
    "            S_inv = np.linalg.inv(np.sqrt(Si))\n",
    "  \n",
    "            # get shape of V\n",
    "            d = V.shape[0]\n",
    "            zeromat = np.zeros(d)\n",
    "\n",
    "            # generate true effect vector\n",
    "            if d > 1:\n",
    "                sim = np.random.multivariate_normal(zeromat, np.eye(d) + ri * S_inv @ V @ S_inv)\n",
    "            else:\n",
    "                sim = np.random.normal(zeromat, np.eye(d) + ri * S_inv @ V @ S_inv)\n",
    "            \n",
    "            # Append to vector of effects\n",
    "            zhat_vec[i, :] = sim\n",
    "        \n",
    "\n",
    "        print(\"Effect Vectors Simulated!\")\n",
    "        \n",
    "        self.snp = np.arange(1, N+1, 1)\n",
    "        self.pos = np.arange(1, N+1, 1)\n",
    "        self.z = zhat_vec\n",
    "\n",
    "    def neg_logll_grad(self, V, z = None, S = None, u = None, r = None, f = None):\n",
    "        \n",
    "        # ============================================ #\n",
    "        # returns negative log likelihood and negative\n",
    "        # of the gradient\n",
    "        # ============================================ #\n",
    "        \n",
    "        z = self.z if z is None else z\n",
    "        S = self.S if S is None else S\n",
    "        u = self.u if u is None else u\n",
    "        r = self.r if r is None else r\n",
    "        f = self.f if f is None else f\n",
    "\n",
    "        # Unflatten V into a matrix\n",
    "        d = S[0].shape[0]\n",
    "        V = hp.return_to_symmetric(V, d)\n",
    "        Gvec = np.zeros((d, d))\n",
    "        \n",
    "        N = len(S)\n",
    "        log_ll = 0\n",
    "        \n",
    "        # Normalizing variables\n",
    "        # V = V * N\n",
    "        V_norm = V/N\n",
    "        for i in range(N):\n",
    "            \n",
    "            Si = S[i]\n",
    "            zi = z[i, :].reshape((d, 1))\n",
    "            ui = u[i]\n",
    "            ri = r[i]\n",
    "            \n",
    "            \n",
    "            fi = f[i]  if f is not None else None\n",
    "\n",
    "            d, ddash = Si.shape\n",
    "            assert d == ddash # Each S has to be a square matrix\n",
    "            \n",
    "            # normalizing variables using allele frequency\n",
    "#             normalizer = 2 * fi  * (1 - fi) if fi is not None else 1.0\n",
    "#             z = np.sqrt(normalizer) * thetai\n",
    "#             Si = normalizer * Si\n",
    "            \n",
    "            Si_inv = np.linalg.inv(np.sqrt(Si))\n",
    "            sigma_exp = np.eye(d) + ri * Si_inv @ V_norm @ Si_inv\n",
    "      \n",
    "            # calculate log likelihood\n",
    "            log_ll_add = -(d/2) * np.log(2 * np.pi)\n",
    "            dit_sv = np.linalg.det(sigma_exp)\n",
    "            dit_sv = 1e-6 if dit_sv < 0 else dit_sv\n",
    "            log_ll_add += -(1/2) * np.log(dit_sv)\n",
    "            log_ll_add += -(1/2) * np.trace(zi @ zi.T @ np.linalg.inv(sigma_exp))\n",
    "            log_ll_add *= 1/ui\n",
    "            \n",
    "            if np.isnan(log_ll_add) == False:\n",
    "                log_ll += log_ll_add\n",
    "            \n",
    "            # calculate gradient\n",
    "            G = (-1/2) * (sigma_exp)\n",
    "            G = G @ (ri * Si_inv @ Si_inv)\n",
    "            G += zi @ zi.T @ (-(sigma_exp) \\\n",
    "                                 @ (ri * Si_inv @ Si_inv) \\\n",
    "                                 @ (sigma_exp))\n",
    "            if np.any(np.isnan(G)) == False:\n",
    "                Gvec += G\n",
    "\n",
    "        Gvec = hp.extract_upper_triangle(Gvec)\n",
    "        return -log_ll , -Gvec\n",
    "\n",
    "\n",
    "    def solve(self,\n",
    "              z = None, \n",
    "              S = None,\n",
    "              u = None,\n",
    "              r = None,\n",
    "              f = None,\n",
    "              neg_logll_grad = None,\n",
    "              est_init = None,\n",
    "              printout = True):\n",
    "        \n",
    "        # inherit parameters from the class if they aren't defined\n",
    "        z = self.z if (z is None) else z\n",
    "        S = self.S if S is None else S\n",
    "        u = self.u if u is None else u\n",
    "        r = self.r if r is None else r\n",
    "        f = self.f if f is None else f\n",
    "        neg_logll_grad = self.neg_logll_grad if neg_logll_grad is None else neg_logll_grad\n",
    "\n",
    "        # == Solves our MLE problem == #\n",
    "        n, m = z.shape\n",
    "        \n",
    "        if est_init is not None:\n",
    "            # Shape of initial varcov guess\n",
    "            rowstrue = est_init.shape[0] == m\n",
    "            colstrue = est_init.shape[1] == m\n",
    "\n",
    "            if rowstrue & colstrue:\n",
    "                pass\n",
    "            else:\n",
    "                if printout == True:\n",
    "                    print(\"Warning: Initial Estimate given is not of the proper dimension\")\n",
    "                    print(\"Making 'optimal' matrix\")\n",
    "                    print(\"=================================================\")\n",
    "                \n",
    "                est_init = np.zeros((m, m))\n",
    "        else:\n",
    "            if printout == True:\n",
    "                print(\"No initial guess provided.\")\n",
    "                print(\"Making 'optimal' matrix\")\n",
    "                print(\"=================================================\")\n",
    "            \n",
    "            est_init = np.zeros((m, m))\n",
    "            \n",
    "        \n",
    "        # exporting for potential later reference\n",
    "        self.est_init = est_init\n",
    "\n",
    "        # extract array from est init\n",
    "        est_init_array = hp.extract_upper_triangle(est_init) \n",
    "        \n",
    "        bounds = hp.extract_bounds(m)\n",
    "\n",
    "        result = minimize(\n",
    "            neg_logll_grad, \n",
    "            est_init_array,\n",
    "            jac = True,\n",
    "            args = (z, S, u, r, f),\n",
    "            bounds = bounds,\n",
    "            method = 'L-BFGS-B'\n",
    "        )\n",
    "        \n",
    "        output_matrix = hp.return_to_symmetric(result.x, m)\n",
    "        \n",
    "        # re-normnalizing output matrix\n",
    "        output_matrix = output_matrix / n\n",
    "        \n",
    "        self.output_matrix = output_matrix\n",
    "        \n",
    "        return output_matrix, result \n",
    "\n",
    "    def jackknife_se(self,\n",
    "                  theta  = None, S = None,\n",
    "                  r = None, u = None,\n",
    "                  blocksize = 1):\n",
    "\n",
    "        # Simple jackknife estimator for SE\n",
    "        # Ref: https://github.com/bulik/ldsc/blob/aa33296abac9569a6422ee6ba7eb4b902422cc74/ldscore/jackknife.py#L231\n",
    "        # Default value of blocksize = 1 is the normal jackknife\n",
    "\n",
    "        theta = self.theta if (theta is None) else theta\n",
    "        S = self.S if (S is None) else S\n",
    "        r = self.r if (r is None) else r\n",
    "        u = self.u if (u is None) else u\n",
    "\n",
    "        \n",
    "        assert theta.shape[0] == S.shape[0]\n",
    "\n",
    "        nobs = theta.shape[0]\n",
    "        \n",
    "        estimates_jk = []\n",
    "        \n",
    "        start_idx = 0\n",
    "        while True:\n",
    "            \n",
    "            end_idx = start_idx + blocksize\n",
    "            end_idx_cond = end_idx <= theta.shape[0]\n",
    "            \n",
    "            # remove blocks of observations\n",
    "\n",
    "            vars_jk = []\n",
    "\n",
    "            for var in [theta, S, r, u]:\n",
    "\n",
    "                var_jk = delete_obs_jk(var, start_idx, end_idx,\n",
    "                                       end_idx_cond)\n",
    "                vars_jk.append(var_jk)\n",
    "            \n",
    "            if start_idx < theta.shape[0]:\n",
    "                # Get our estimate\n",
    "                output_matrix, _ = self.solve(theta = vars_jk[0],\n",
    "                                              S = vars_jk[1],\n",
    "                                              r = vars_jk[2],\n",
    "                                              u = vars_jk[3],\n",
    "                                              printout = False,\n",
    "                                              est_init = self.est_init)\n",
    "\n",
    "                estimates_jk.append(output_matrix)\n",
    "\n",
    "                start_idx += blocksize\n",
    "            else:\n",
    "                break\n",
    "            \n",
    "        estimates_jk = np.array(estimates_jk)\n",
    "        full_est = self.output_matrix\n",
    "        \n",
    "        # calculate pseudo-values\n",
    "        n_blocks = int(nobs/blocksize)\n",
    "        pseudovalues = n_blocks * full_est - (n_blocks - 1) * estimates_jk\n",
    "        \n",
    "        # calculate jackknife se\n",
    "        pseudovalues = pseudovalues.reshape((n_blocks, theta.shape[1] * theta.shape[1]))\n",
    "        jknife_cov = np.cov(pseudovalues.T, ddof=1) / n_blocks\n",
    "        jknife_var = np.diag(jknife_cov)\n",
    "        jknife_se = np.sqrt(jknife_var)\n",
    "    \n",
    "        jknife_se  = jknife_se.reshape((theta.shape[1], theta.shape[1]))\n",
    "        \n",
    "        return jknife_se  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "S_size=  int(N/2)\n",
    "S = np.array([np.array([[.5, 0], [0, .8]]),\n",
    "    np.array([[0.5, 0], [0, 0.8]])] * S_size )/N\n",
    "V = np.identity(2) * 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning there is no value for z. Maybe consider simulating it\n",
      "No value for U given. Generating a vector of ones (all SNPs weighted equally)\n",
      "No value for r given. Generating a vector of ones for r\n",
      "Warning: No value given for allele frequencies. Some parameters won't be noramlized.\n",
      "Simulated LD scores!\n",
      "Effect Vectors Simulated!\n"
     ]
    }
   ],
   "source": [
    "model = sibreg(S = S)\n",
    "model.simdata(V, N, simr = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(728.9845921999076, array([9.33808771e+09, 7.38525333e+07, 2.58037617e+09]))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vin = hp.extract_upper_triangle(V)\n",
    "model.neg_logll_grad(Vin * N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No initial guess provided.\n",
      "Making 'optimal' matrix\n",
      "=================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[ 1.00000000e-08, -6.01886222e-03],\n",
       "        [-6.01886222e-03,  1.00000000e-08]]),       fun: -507.18498187357187\n",
       "  hess_inv: <3x3 LbfgsInvHessProduct with dtype=float64>\n",
       "       jac: array([ 614699.10539998, -412926.16648053, 1002942.66333233])\n",
       "   message: b'ABNORMAL_TERMINATION_IN_LNSRCH'\n",
       "      nfev: 63\n",
       "       nit: 2\n",
       "    status: 2\n",
       "   success: False\n",
       "         x: array([ 1.00000000e-06, -6.01886222e-01,  1.00000000e-06]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.solve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-51.6393107]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def logll(V):\n",
    "\n",
    "    Vin = hp.extract_upper_triangle(V)\n",
    "    logll = -model.neg_logll_grad(Vin)[0]\n",
    "    \n",
    "    return logll\n",
    "\n",
    "def gradlogll(V):\n",
    "\n",
    "    Vin = hp.extract_upper_triangle(V)\n",
    "    grad = -model.neg_logll_grad(Vin)[1]\n",
    "    \n",
    "    return grad\n",
    "\n",
    "def num_grad(V, row = 0, col = 0, dx = 1e-6):\n",
    "    \n",
    "    V1 = np.copy(V)\n",
    "    V1[row, col] += dx\n",
    "    \n",
    "    derivative = (logll(V1) - logll(V))/dx\n",
    "    \n",
    "    return derivative\n",
    "\n",
    "num_grad(V, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1548235.57487135,    11072.70223056,  -556545.8429542 ])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradlogll(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradients dont align!\n",
    "\n",
    "Let's see if the likelihood is correctly specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-359.25346977141857"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Si = S[0]\n",
    "ri = model.r[0]\n",
    "S_inv = np.linalg.inv(np.sqrt(Si))\n",
    "logll = scipy.stats.multivariate_normal.logpdf(model.z, \n",
    "                                       mean = np.zeros(2),\n",
    "                                      cov = np.eye(2) + ri * S_inv @ V/N @ S_inv)\n",
    "logll.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-341.5283683896606"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vin = hp.extract_upper_triangle(V)\n",
    "-model.neg_logll_grad(Vin)[0][0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
