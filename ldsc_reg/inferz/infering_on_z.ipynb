{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helperfuncs as hp\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import comb\n",
    "from scipy.misc import derivative\n",
    "import scipy.stats\n",
    "from numba import jit, njit, vectorize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the PDF and the Log Likelihoods\n",
    "\n",
    "The likelihood for a SNP $i$ is:\n",
    "\n",
    "$$\n",
    "l_i = -\\frac{d}{2} log (2 \\pi) - \\frac{1}{2} log ( |I + r_i S_i^{-1/2} V S_i^{-1/2}| ) - \\frac{1}{2} z_i^T (I + r_i S_i^{-1/2} V S_i^{-1/2}) ^{-1} z_i\n",
    "$$\n",
    "\n",
    "And its derivative:\n",
    "\n",
    "$$\n",
    "\\frac{dl}{dV} = r_i S^{-1/2} \\Sigma_i^{-1} (\\Sigma - z_i z_i^T) \\Sigma_i^{-1} S^{-1/2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sibreg():\n",
    "    \n",
    "    def __init__(self, S, z = None, u = None, r = None, f = None):\n",
    "        \n",
    "        if S.ndim > 1:\n",
    "            for s in S:\n",
    "                n, m = s.shape\n",
    "                assert n == m\n",
    "\n",
    "        if z is None:\n",
    "            print(\"Warning there is no value for z. Maybe consider simulating it\")\n",
    "        if u is None:\n",
    "            print(\"No value for U given. Generating a vector of ones (all SNPs weighted equally)\")\n",
    "            u = np.ones(S.shape[0])\n",
    "        if r is None:\n",
    "            print(\"No value for r given. Generating a vector of ones for r\")\n",
    "            r = np.ones(S.shape[0])\n",
    "        if f is None:\n",
    "            print(\"Warning: No value given for allele frequencies. Some parameters won't be noramlized.\")\n",
    "        \n",
    "        self.z = None if z is None else z[~np.any(np.isnan(z), axis = 1)]\n",
    "        self.S = S[~np.any(np.isnan(S), axis = (1, 2))]\n",
    "        self.u = u[~np.isnan(u)]\n",
    "        self.r = r[~np.isnan(r)]\n",
    "        self.f = None if f is None else f[~np.isnan(f)]\n",
    "    \n",
    "\n",
    "    def simdata(self, V,  N, simr = False):\n",
    "        \"\"\"\n",
    "        Simulates data for z scores.\n",
    "        \n",
    "        Inputs:\n",
    "        V = varcov matrix of true effects\n",
    "        N = Number of obs/SNPs to generate\n",
    "        simr = boolean indicating if we want\n",
    "                to simulate ldscores\n",
    "                \n",
    "        \n",
    "        Outputs:\n",
    "        None\n",
    "        \n",
    "        - It creates an object within the class\n",
    "        called z\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        S = self.S\n",
    "        \n",
    "        if simr:\n",
    "            self.r = np.random.uniform(low=1, high=5, size=N)\n",
    "            print(\"Simulated LD scores!\")\n",
    "        \n",
    "        r = self.r\n",
    "\n",
    "        zhat_vec = np.empty((N, V.shape[1]))\n",
    "        for i in range(N):\n",
    "            \n",
    "            Si = S[i]\n",
    "            ri = r[i]\n",
    "            \n",
    "            V = np.array(V)\n",
    "            Si = np.array(Si)\n",
    "            S_inv = np.linalg.inv(np.sqrt(Si))\n",
    "  \n",
    "            # get shape of V\n",
    "            d = V.shape[0]\n",
    "            zeromat = np.zeros(d)\n",
    "\n",
    "            # generate true effect vector\n",
    "            if d > 1:\n",
    "                sim = np.random.multivariate_normal(zeromat, np.eye(d) + ri * S_inv @ V @ S_inv)\n",
    "            else:\n",
    "                sim = np.random.normal(zeromat, np.eye(d) + ri * S_inv @ V @ S_inv)\n",
    "            \n",
    "            # Append to vector of effects\n",
    "            zhat_vec[i, :] = sim\n",
    "        \n",
    "\n",
    "        print(\"Effect Vectors Simulated!\")\n",
    "        \n",
    "        self.snp = np.arange(1, N+1, 1)\n",
    "        self.pos = np.arange(1, N+1, 1)\n",
    "        self.z = zhat_vec\n",
    "        \n",
    "    def _log_ll(self, V, z, S, u, r, f):\n",
    "        \n",
    "        \"\"\"\n",
    "        Returns the log likelihood matrix for a given SNP i as formulated by:\n",
    "        \n",
    "        l_i = -\\frac{d}{2} log (2 \\pi) - \\frac{1}{2} log ( |I + r_i S_i^{-1/2} V S_i^{-1/2}| ) -\n",
    "                \\frac{1}{2} z_i^T (I + r_i S_i^{-1/2} V S_i^{-1/2}) ^{-1} z_i\n",
    "                \n",
    "        Inputs:\n",
    "        V = dxd numpy matrix\n",
    "        z = dx1 numpy matrix\n",
    "        S = dxd numpy matrix\n",
    "        u = 1 numpy matrix\n",
    "        r = 1 numpy matrix\n",
    "        f = 1 numpy matrix\n",
    "        \n",
    "        Outputs:\n",
    "        logll = 1 dimensional matrix \n",
    "        \"\"\"\n",
    "        \n",
    "        S_inv_root = hp.calc_inv_root(S)\n",
    "        Sigma = np.identity(S.shape[0])+r*np.dot(S_inv_root.dot(V),S_inv_root)\n",
    "        logdet = np.linalg.slogdet(Sigma)\n",
    "        Sigma_inv = np.linalg.inv(Sigma)\n",
    "        z = z.reshape(z.shape[0],1)\n",
    "        L = logdet[0]*logdet[1]+np.dot(z.T,Sigma_inv.dot(z))\n",
    "        return L\n",
    "    \n",
    "    def _grad_ll_v(self, V, z, S, u, r, f):\n",
    "        \n",
    "        \"\"\"\n",
    "        Returns the gradient of the log likelihood wrt V for a given SNP i as formulated by:\n",
    "        \n",
    "        \\frac{dl}{dV} = S^{-1/2} \\Sigma_i^{-1} (\\Sigma - z_i z_i^T) \\Sigma_i^{-1} S^{-1/2}\n",
    "                \n",
    "        Inputs:\n",
    "        V = dxd numpy matrix\n",
    "        z = dx1 numpy matrix\n",
    "        S = dxd numpy matrix\n",
    "        u = 1 numpy matrix\n",
    "        r = 1 numpy matrix\n",
    "        f = 1 numpy matrix\n",
    "        \n",
    "        Outputs:\n",
    "        grad_ll_v = dxd matrix \n",
    "        \"\"\"\n",
    "        \n",
    "        S_inv_root = hp.calc_inv_root(S)\n",
    "        Sigma = np.identity(S.shape[0])+r*np.dot(S_inv_root.dot(V),S_inv_root)\n",
    "        Sigma_inv = np.linalg.inv(Sigma)\n",
    "        z = z.reshape(z.shape[0],1)\n",
    "        SSigma_inv = S_inv_root.dot(Sigma_inv)\n",
    "        g = r * SSigma_inv.dot(np.dot(Sigma-z.dot(z.T),SSigma_inv.T))\n",
    "        return g\n",
    "    \n",
    "    def _num_grad_V(self, V, z, S, u, r, f):\n",
    "        \"\"\"\n",
    "        Returns numerical gradient vector of self._log_ll\n",
    "        Mostly meant to check if self._grad_ll_v is working\n",
    "        properly\n",
    "        \n",
    "        Inputs:\n",
    "        V = dxd numpy matrix\n",
    "        z = dx1 numpy matrix\n",
    "        S = dxd numpy matrix\n",
    "        u = 1 numpy matrix\n",
    "        r = 1 numpy matrix\n",
    "        f = 1 numpy matrix\n",
    "        \n",
    "        Outputs:\n",
    "        g = dxd matrix \n",
    "        \"\"\"\n",
    "        \n",
    "        g = np.zeros(V.shape)\n",
    "        for i in range(0,V.shape[0]):\n",
    "            for j in range(0,V.shape[1]):\n",
    "                dV = np.zeros((V.shape))\n",
    "                dV[i,j] = 10 ** (-6)\n",
    "                V_upper = V+dV\n",
    "                V_lower = V-dV\n",
    "                g[i,j] = (self._log_ll(V_upper, z, S, u, r, f) - \\\n",
    "                          self._log_ll(V_lower, z, S, u, r, f)) / (2 * 10 ** (-6))\n",
    "        return g\n",
    "\n",
    "\n",
    "    def neg_logll_grad(self, V, \n",
    "                       z = None, S = None, \n",
    "                       u = None, r = None, \n",
    "                       f = None,\n",
    "                       logllfunc = None,\n",
    "                       gradfunc = None):\n",
    "        \n",
    "        \"\"\"\n",
    "        Returns the loglikelihood and its gradient wrt V for a given SNP i as formulated by:\n",
    "        \n",
    "        l_i = -\\frac{d}{2} log (2 \\pi) - \\frac{1}{2} log ( |I + r_i S_i^{-1/2} V S_i^{-1/2}| ) -\n",
    "                \\frac{1}{2} z_i^T (I + r_i S_i^{-1/2} V S_i^{-1/2}) ^{-1} z_i\n",
    "        \n",
    "        and\n",
    "        \n",
    "        \\frac{dl}{dV} = S^{-1/2} \\Sigma_i^{-1} (\\Sigma - z_i z_i^T) \\Sigma_i^{-1} S^{-1/2}\n",
    "                \n",
    "        Inputs:\n",
    "        V = dxd numpy matrix\n",
    "        z = dxN numpy matrix\n",
    "        S = dxd numpy matrix\n",
    "        u = 1 numpy matrix\n",
    "        r = 1 numpy matrix\n",
    "        f = 1 numpy matrix\n",
    "        logllfunc = function which calculates logll\n",
    "                    (uses self._log_ll by default)\n",
    "        gradfunc = function which calculated grad of logll\n",
    "                    (uses self._grad_ll_v by default)\n",
    "        \n",
    "        Outputs:\n",
    "        -log_ll = 1x1 scalar\n",
    "        -Gvec = dxd numpy matrix\n",
    "        \"\"\"\n",
    "        \n",
    "        z = self.z if z is None else z\n",
    "        S = self.S if S is None else S\n",
    "        u = self.u if u is None else u\n",
    "        r = self.r if r is None else r\n",
    "        f = self.f if f is None else f\n",
    "        logllfunc = self._log_ll if logllfunc is None else logllfunc\n",
    "        gradfunc = self._grad_ll_v if gradfunc is None else grandfunc\n",
    "\n",
    "        # Unflatten V into a matrix\n",
    "        d = S[0].shape[0]\n",
    "        V = hp.return_to_symmetric(V, d)\n",
    "        Gvec = np.zeros((d, d))\n",
    "        \n",
    "        N = len(S)\n",
    "        log_ll = 0\n",
    "        \n",
    "        # Normalizing variables\n",
    "        V_norm = V/N\n",
    "        for i in range(N):\n",
    "            \n",
    "            Si = S[i]\n",
    "            zi = z[i, :].reshape((d, 1))\n",
    "            ui = u[i]\n",
    "            ri = r[i]\n",
    "            \n",
    "            \n",
    "            fi = f[i]  if f is not None else None\n",
    "\n",
    "            d, ddash = Si.shape\n",
    "            assert d == ddash # Each S has to be a square matrix\n",
    "            \n",
    "            log_ll += logllfunc(V_norm, zi, Si, ui, ri, fi)\n",
    "            Gvec += gradfunc(V_norm, zi, Si, ui, ri, fi)\n",
    "\n",
    "        Gvec = hp.extract_upper_triangle(Gvec)\n",
    "        return -log_ll , -Gvec\n",
    "\n",
    "\n",
    "    def solve(self,\n",
    "              z = None, \n",
    "              S = None,\n",
    "              u = None,\n",
    "              r = None,\n",
    "              f = None,\n",
    "              neg_logll_grad = None,\n",
    "              est_init = None,\n",
    "              printout = True):\n",
    "        \n",
    "        \"\"\"\n",
    "        Solves the ldsc problem of infering the V matrix\n",
    "                \n",
    "        Inputs:\n",
    "        z = Nx1 numpy matrix\n",
    "        S = dxd numpy matrix\n",
    "        u = 1 numpy matrix\n",
    "        r = 1 numpy matrix\n",
    "        f = 1 numpy matrix\n",
    "        \n",
    "        Outputs:\n",
    "        output_matrix = dxd numpy matrix\n",
    "        result = result of scipy solver \n",
    "        \"\"\"\n",
    "        \n",
    "        # inherit parameters from the class if they aren't defined\n",
    "        z = self.z if z is None else z\n",
    "        S = self.S if S is None else S\n",
    "        u = self.u if u is None else u\n",
    "        r = self.r if r is None else r\n",
    "        f = self.f if f is None else f\n",
    "        neg_logll_grad = self.neg_logll_grad if neg_logll_grad is None else neg_logll_grad\n",
    "\n",
    "        # == Solves our MLE problem == #\n",
    "        n, m = z.shape\n",
    "        \n",
    "        if est_init is not None:\n",
    "            # Shape of initial varcov guess\n",
    "            rowstrue = est_init.shape[0] == m\n",
    "            colstrue = est_init.shape[1] == m\n",
    "\n",
    "            if rowstrue & colstrue:\n",
    "                pass\n",
    "            else:\n",
    "                if printout == True:\n",
    "                    print(\"Warning: Initial Estimate given is not of the proper dimension\")\n",
    "                    print(\"Making 'optimal' matrix\")\n",
    "                    print(\"=================================================\")\n",
    "                \n",
    "                est_init = np.zeros((m, m))\n",
    "        else:\n",
    "            if printout == True:\n",
    "                print(\"No initial guess provided.\")\n",
    "                print(\"Making 'optimal' matrix\")\n",
    "                print(\"=================================================\")\n",
    "            \n",
    "            est_init = np.zeros((m, m))\n",
    "            \n",
    "        \n",
    "        # exporting for potential later reference\n",
    "        self.est_init = est_init\n",
    "\n",
    "        # extract array from est init\n",
    "        est_init_array = hp.extract_upper_triangle(est_init) \n",
    "        \n",
    "        bounds = hp.extract_bounds(m)     \n",
    "\n",
    "        result = minimize(\n",
    "            neg_logll_grad, \n",
    "            est_init_array,\n",
    "            jac = True,\n",
    "            args = (z, S, u, r, f),\n",
    "            bounds = bounds,\n",
    "            method = 'L-BFGS-B'\n",
    "        )\n",
    "        \n",
    "        output_matrix = hp.return_to_symmetric(result.x, m)\n",
    "        \n",
    "        # re-normnalizing output matrix\n",
    "#         output_matrix = output_matrix / n\n",
    "        \n",
    "        self.output_matrix = output_matrix\n",
    "        \n",
    "        return output_matrix, result \n",
    "\n",
    "    def jackknife_se(self,\n",
    "                  theta  = None, S = None,\n",
    "                  r = None, u = None,\n",
    "                  blocksize = 1):\n",
    "\n",
    "        # Simple jackknife estimator for SE\n",
    "        # Ref: https://github.com/bulik/ldsc/blob/aa33296abac9569a6422ee6ba7eb4b902422cc74/ldscore/jackknife.py#L231\n",
    "        # Default value of blocksize = 1 is the normal jackknife\n",
    "\n",
    "        theta = self.theta if (theta is None) else theta\n",
    "        S = self.S if (S is None) else S\n",
    "        r = self.r if (r is None) else r\n",
    "        u = self.u if (u is None) else u\n",
    "\n",
    "        \n",
    "        assert theta.shape[0] == S.shape[0]\n",
    "\n",
    "        nobs = theta.shape[0]\n",
    "        \n",
    "        estimates_jk = []\n",
    "        \n",
    "        start_idx = 0\n",
    "        while True:\n",
    "            \n",
    "            end_idx = start_idx + blocksize\n",
    "            end_idx_cond = end_idx <= theta.shape[0]\n",
    "            \n",
    "            # remove blocks of observations\n",
    "\n",
    "            vars_jk = []\n",
    "\n",
    "            for var in [theta, S, r, u]:\n",
    "\n",
    "                var_jk = delete_obs_jk(var, start_idx, end_idx,\n",
    "                                       end_idx_cond)\n",
    "                vars_jk.append(var_jk)\n",
    "            \n",
    "            if start_idx < theta.shape[0]:\n",
    "                # Get our estimate\n",
    "                output_matrix, _ = self.solve(theta = vars_jk[0],\n",
    "                                              S = vars_jk[1],\n",
    "                                              r = vars_jk[2],\n",
    "                                              u = vars_jk[3],\n",
    "                                              printout = False,\n",
    "                                              est_init = self.est_init)\n",
    "\n",
    "                estimates_jk.append(output_matrix)\n",
    "\n",
    "                start_idx += blocksize\n",
    "            else:\n",
    "                break\n",
    "            \n",
    "        estimates_jk = np.array(estimates_jk)\n",
    "        full_est = self.output_matrix\n",
    "        \n",
    "        # calculate pseudo-values\n",
    "        n_blocks = int(nobs/blocksize)\n",
    "        pseudovalues = n_blocks * full_est - (n_blocks - 1) * estimates_jk\n",
    "        \n",
    "        # calculate jackknife se\n",
    "        pseudovalues = pseudovalues.reshape((n_blocks, theta.shape[1] * theta.shape[1]))\n",
    "        jknife_cov = np.cov(pseudovalues.T, ddof=1) / n_blocks\n",
    "        jknife_var = np.diag(jknife_cov)\n",
    "        jknife_se = np.sqrt(jknife_var)\n",
    "    \n",
    "        jknife_se  = jknife_se.reshape((theta.shape[1], theta.shape[1]))\n",
    "        \n",
    "        return jknife_se  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "S_size = int(N/2)\n",
    "S = np.array([np.array([[.5, 0], [0, .8]]),\n",
    "    np.array([[0.5, 0], [0, 0.8]])] * S_size )/N\n",
    "V = np.identity(2) * 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning there is no value for z. Maybe consider simulating it\n",
      "No value for U given. Generating a vector of ones (all SNPs weighted equally)\n",
      "No value for r given. Generating a vector of ones for r\n",
      "Warning: No value given for allele frequencies. Some parameters won't be noramlized.\n",
      "Simulated LD scores!\n",
      "Effect Vectors Simulated!\n"
     ]
    }
   ],
   "source": [
    "model = sibreg(S = S)\n",
    "model.simdata(V, N, simr = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-1281.7558863]]), array([-6.82758698, 11.53012504,  3.5616072 ]))"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vin = hp.extract_upper_triangle(V)\n",
    "model.neg_logll_grad(Vin * N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing derivatives\n",
    "aderiv = model._grad_ll_v(V, model.z[0, :], model.S[0], \n",
    "                 model.u[0], model.r[0], \n",
    "                 model.f)\n",
    "\n",
    "nderiv = model._num_grad_V(V, model.z[0, :], model.S[0], \n",
    "                 model.u[0], model.r[0], \n",
    "                 model.f)\n",
    "\n",
    "np.allclose(aderiv, nderiv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      fun: array([[-82059.8819275]])\n",
      " hess_inv: <3x3 LbfgsInvHessProduct with dtype=float64>\n",
      "      jac: array([1.14884943e+08, 1.72073068e+08, 2.57728481e+08])\n",
      "  message: b'ABNORMAL_TERMINATION_IN_LNSRCH'\n",
      "     nfev: 53\n",
      "      nit: 3\n",
      "   status: 2\n",
      "  success: False\n",
      "        x: array([ 58.56173418, -39.16041368,  25.99068745])\n"
     ]
    }
   ],
   "source": [
    "# solving\n",
    "output, result = model.solve(est_init = V*N)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# palying\n",
    "\n",
    "n=100\n",
    "scale = -3\n",
    "S = np.array([[10**scale,0.5*10**scale],[0.5*10**scale,10**scale]])\n",
    "V = np.array([[10**(scale-1),0.5*10**(scale-1)],[0.5*10**(scale-1),10**(scale-1)]])\n",
    "z = np.random.multivariate_normal(np.zeros((2)),S+V,size=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = np.array([S.tolist()] * n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_inv_root(S):\n",
    "    S_eig = np.linalg.eig(S)\n",
    "    l = np.zeros(S.shape)\n",
    "    np.fill_diagonal(l,np.power(S_eig[0],-0.5))\n",
    "    S_inv_root = S_eig[1].dot(np.dot(l,S_eig[1].T)) \n",
    "    return S_inv_root\n",
    "\n",
    "def likelihood(z,S,V):\n",
    "    S_inv_root = calc_inv_root(S)\n",
    "    Sigma = np.identity(S.shape[0])+np.dot(S_inv_root.dot(V),S_inv_root)\n",
    "    logdet = np.linalg.slogdet(Sigma)\n",
    "    Sigma_inv = np.linalg.inv(Sigma)\n",
    "    z = z.reshape(z.shape[0],1)\n",
    "    L = logdet[0]*logdet[1]+np.dot(z.T,Sigma_inv.dot(z))\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1917612]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likelihood(z[0], S[0], V)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}