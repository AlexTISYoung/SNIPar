{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helperfuncs as hp\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import comb\n",
    "import sympy as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the PDF and the Log Likelihoods\n",
    "\n",
    "Consider a random variable $z_i$ of length $d$ given by:\n",
    "\n",
    "$$\n",
    "z_i \\sim \\mathcal{N}(0, I + r_i S_i^{-1/2} V S_i^{-1/2})\n",
    "$$\n",
    "\n",
    "The log likelihood $l$ for a SNP $i$ is given by:\n",
    "\n",
    "$$\n",
    "l = -\\frac{d}{2} log(2 \\pi) - \\frac{1}{2} log |I + r_i S_i^{-1/2} V S_i^{-1/2} | - \\frac{1}{2} z_i (I + r_i S_i^{-1/2} V S_i^{-1/2}) ^ {-1}z_i^T\n",
    "$$\n",
    "\n",
    "Its derivative with respect to V is given by:\n",
    "\n",
    "$$\n",
    "\\frac{dl}{dV} = -\\frac{1}{2}(I + r_i S_i^{-1/2} V S_i^{-1/2}) ^ {-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lastpor_ll(z, r, S, V):\n",
    "    \n",
    "    d = z.shape[1]\n",
    "    \n",
    "    return -(1/2) * \\\n",
    "        (z * (sp.Identity(d) + r * (S ** (1/2)).inv() * V * (S ** (1/2)).inv()).inv() * z.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - 0.5 z \\left(r S^{-0.5} V S^{-0.5} + \\mathbb{I}\\right)^{-1} z^{T}$"
      ],
      "text/plain": [
       "-0.5*z*(r*S**(-0.5)*V*S**(-0.5) + I)**(-1)*z.T"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = sp.symbols(\"d\", real = True)\n",
    "z = sp.MatrixSymbol(\"z\", 1, d)\n",
    "r = sp.symbols(\"r\", real = True)\n",
    "S = sp.MatrixSymbol(\"S\", d, d)\n",
    "V = sp.MatrixSymbol(\"V\", d, d)\n",
    "\n",
    "lastpor_ll(z, r, S, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - 0.5 z \\left(r S^{-0.5} V S^{-0.5} + \\mathbb{I}\\right)^{-1} - 0.5 z \\left(r \\left(S^{T}\\right)^{-0.5} V^{T} \\left(S^{T}\\right)^{-0.5} + \\mathbb{I}\\right)^{-1}$"
      ],
      "text/plain": [
       "-0.5*z*(r*S**(-0.5)*V*S**(-0.5) + I)**(-1) - 0.5*z*(r*S.T**(-0.5)*V.T*S.T**(-0.5) + I)**(-1)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.diff(lastpor_ll(z, r, S, V), z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sibreg():\n",
    "    \n",
    "    def __init__(self, S, theta = None, u = None, r = None, f = None):\n",
    "        \n",
    "        if S.ndim > 1:\n",
    "            for s in S:\n",
    "                n, m = s.shape\n",
    "                assert n == m\n",
    "\n",
    "        if theta is None:\n",
    "            print(\"Warning there is no value for theta. Maybe consider simulating it\")\n",
    "        if u is None:\n",
    "            print(\"No value for U given. Generating a vector of ones (all SNPs weighted equally)\")\n",
    "            u = np.ones(S.shape[0])\n",
    "        if r is None:\n",
    "            print(\"No value for r given. Generating a vector of ones for r\")\n",
    "            r = np.ones(S.shape[0])\n",
    "        if f is None:\n",
    "            print(\"Warning: No value given for allele frequencies. Some parameters won't be noramlized.\")\n",
    "        \n",
    "        self.theta = None if theta is None else theta[~np.any(np.isnan(theta), axis = 1)]\n",
    "        self.S = S[~np.any(np.isnan(S), axis = (1, 2))]\n",
    "        self.u = u[~np.isnan(u)]\n",
    "        self.r = r[~np.isnan(r)]\n",
    "        self.f = None if f is None else f[~np.isnan(f)]\n",
    "    \n",
    "\n",
    "    def simdata(self, V,  N, simr = False):\n",
    "        \n",
    "        # Simulated data (theta hats) as per section 7.1\n",
    "        # V = varcov matrix of true effects\n",
    "        # N = Number of obs/SNPs to generate\n",
    "        \n",
    "        S = self.S\n",
    "        \n",
    "        if simr:\n",
    "            self.r = np.random.uniform(low=1, high=5, size=N)\n",
    "            print(\"Simulated LD scores!\")\n",
    "        \n",
    "        r = self.r\n",
    "\n",
    "        thetahat_vec = np.empty((N, V.shape[1]))\n",
    "        \n",
    "        # make sure they are np arrays\n",
    "        for i in range(N):\n",
    "            \n",
    "            Si = S[i]\n",
    "            ri = r[i]\n",
    "            \n",
    "            V = np.array(V)\n",
    "            Si = np.array(Si)\n",
    "\n",
    "            # get shape of V\n",
    "            d = V.shape[0]\n",
    "            zeromat = np.zeros(d)\n",
    "\n",
    "            # generate true effect vector\n",
    "            if d > 1:\n",
    "                sim = np.random.multivariate_normal(zeromat, Si + ri * V)\n",
    "                if i > (2/3)*N:\n",
    "                    sim = np.random.multivariate_normal(zeromat, Si + ri * V)\n",
    "            else:\n",
    "                sim = np.random.normal(zeromat, Si + ri * V)\n",
    "                if i > (2/3)*N:\n",
    "                    sim = np.random.normal(zeromat, Si + ri * V)\n",
    "            \n",
    "            # Append to vector of effects\n",
    "            thetahat_vec[i, :] = sim\n",
    "        \n",
    "\n",
    "        print(\"Effect Vectors Simulated!\")\n",
    "        \n",
    "        self.snp = np.arange(1, N+1, 1)\n",
    "        self.pos = np.arange(1, N+1, 1)\n",
    "        self.theta = thetahat_vec\n",
    "\n",
    "    def neg_logll_grad(self, V, theta = None, S = None, u = None, r = None, f = None):\n",
    "        \n",
    "        # ============================================ #\n",
    "        # returns negative log likelihood and negative\n",
    "        # of the gradient\n",
    "        # ============================================ #\n",
    "        \n",
    "        theta = self.theta if theta is None else theta\n",
    "        S = self.S if S is None else S\n",
    "        u = self.u if u is None else u\n",
    "        r = self.r if r is None else r\n",
    "        f = self.f if f is None else f\n",
    "\n",
    "        # Unflatten V into a matrix\n",
    "        d = S[0].shape[0]\n",
    "        V = return_to_symmetric(V, d)\n",
    "        Gvec = np.zeros((d, d))\n",
    "        \n",
    "        N = len(S)\n",
    "        log_ll = 0\n",
    "        \n",
    "        # Normalizing variables\n",
    "        # V = V * N\n",
    "        V_norm = V/N\n",
    "        for i in range(N):\n",
    "            \n",
    "            Si = S[i]\n",
    "            thetai = theta[i, :]\n",
    "            ui = u[i]\n",
    "            ri = r[i]\n",
    "            \n",
    "            \n",
    "            fi = f[i]  if f is not None else None\n",
    "\n",
    "            d, ddash = Si.shape\n",
    "            assert d == ddash # Each S has to be a square matrix\n",
    "            \n",
    "            # normalizing variables using allele frequency\n",
    "            normalizer = 2 * fi  * (1 - fi) if fi is not None else 1.0\n",
    "            thetai = np.sqrt(normalizer) * thetai\n",
    "            Si = normalizer * Si\n",
    "      \n",
    "            # calculate log likelihood\n",
    "            log_ll_add = -(d/2) * np.log(2 * np.pi)\n",
    "            dit_sv = np.linalg.det(Si + ri * V_norm)\n",
    "            dit_sv = 1e-6 if dit_sv < 0 else dit_sv\n",
    "            log_ll_add += -(1/2) * np.log(dit_sv)\n",
    "            log_ll_add += -(1/2) * np.trace(np.outer(thetai, thetai) @ np.linalg.inv(Si + ri * V_norm))\n",
    "            log_ll_add *= 1/ui\n",
    "            \n",
    "            if np.isnan(log_ll_add) == False:\n",
    "                log_ll += log_ll_add\n",
    "            \n",
    "            # calculate gradient\n",
    "            SV_inv = np.linalg.inv(Si + ri * V_norm)\n",
    "            G = -(1 / 2) * SV_inv\n",
    "            G += (1 / 2) * np.dot(SV_inv,np.dot(np.outer(thetai, thetai),SV_inv))\n",
    "            G *= 1/ui\n",
    "            \n",
    "            if np.any(np.isnan(G)) == False:\n",
    "                Gvec += G\n",
    "\n",
    "        Gvec = extract_upper_triangle(Gvec)\n",
    "        return -log_ll, -Gvec\n",
    "\n",
    "\n",
    "    def solve(self,\n",
    "              theta = None, \n",
    "              S = None,\n",
    "              u = None,\n",
    "              r = None,\n",
    "              f = None,\n",
    "              neg_logll_grad = None,\n",
    "              est_init = None,\n",
    "              printout = True):\n",
    "        \n",
    "        # inherit parameters from the class if they aren't defined\n",
    "        theta = self.theta if (theta is None) else theta\n",
    "        S = self.S if S is None else S\n",
    "        u = self.u if u is None else u\n",
    "        r = self.r if r is None else r\n",
    "        f = self.f if f is None else f\n",
    "        neg_logll_grad = self.neg_logll_grad if neg_logll_grad is None else neg_logll_grad\n",
    "\n",
    "        # == Solves our MLE problem == #\n",
    "        n, m = theta.shape\n",
    "        \n",
    "        if est_init is not None:\n",
    "            # Shape of initial varcov guess\n",
    "            rowstrue = est_init.shape[0] == m\n",
    "            colstrue = est_init.shape[1] == m\n",
    "\n",
    "            if rowstrue & colstrue:\n",
    "                pass\n",
    "            else:\n",
    "                if printout == True:\n",
    "                    print(\"Warning: Initial Estimate given is not of the proper dimension\")\n",
    "                    print(\"Making 'optimal' matrix\")\n",
    "                    print(\"=================================================\")\n",
    "                \n",
    "                theta_full = theta\n",
    "                S_full = S/n\n",
    "                \n",
    "                theta_var = np.cov(theta_full.T)\n",
    "                S_hat = np.mean(S_full, axis = 0)\n",
    "                est_init = n * (theta_var - S_hat)\n",
    "        else:\n",
    "            if printout == True:\n",
    "                print(\"No initial guess provided.\")\n",
    "                print(\"Making 'optimal' matrix\")\n",
    "                print(\"=================================================\")\n",
    "            \n",
    "            theta_full = theta\n",
    "            S_full = S/n\n",
    "            \n",
    "            theta_var = np.cov(theta_full.T)\n",
    "            S_hat = np.mean(S_full, axis = 0)\n",
    "            est_init = n * (theta_var - S_hat)\n",
    "            \n",
    "        \n",
    "        # exporting for potential later reference\n",
    "        self.est_init = est_init\n",
    "\n",
    "        # extract array from est init\n",
    "        est_init_array = extract_upper_triangle(est_init) \n",
    "        \n",
    "        bounds = extract_bounds(m)\n",
    "\n",
    "        result = minimize(\n",
    "            neg_logll_grad, \n",
    "            est_init_array,\n",
    "            jac = True,\n",
    "            args = (theta, S, u, r, f),\n",
    "            bounds = bounds,\n",
    "            method = 'L-BFGS-B'\n",
    "        )\n",
    "        \n",
    "        output_matrix = return_to_symmetric(result.x, m)\n",
    "        \n",
    "        # re-normnalizing output matrix\n",
    "        output_matrix = output_matrix / n\n",
    "        \n",
    "        self.output_matrix = output_matrix\n",
    "        \n",
    "        return output_matrix, result \n",
    "\n",
    "    def jackknife_se(self,\n",
    "                  theta  = None, S = None,\n",
    "                  r = None, u = None,\n",
    "                  blocksize = 1):\n",
    "\n",
    "        # Simple jackknife estimator for SE\n",
    "        # Ref: https://github.com/bulik/ldsc/blob/aa33296abac9569a6422ee6ba7eb4b902422cc74/ldscore/jackknife.py#L231\n",
    "        # Default value of blocksize = 1 is the normal jackknife\n",
    "\n",
    "        theta = self.theta if (theta is None) else theta\n",
    "        S = self.S if (S is None) else S\n",
    "        r = self.r if (r is None) else r\n",
    "        u = self.u if (u is None) else u\n",
    "\n",
    "        \n",
    "        assert theta.shape[0] == S.shape[0]\n",
    "\n",
    "        nobs = theta.shape[0]\n",
    "        \n",
    "        estimates_jk = []\n",
    "        \n",
    "        start_idx = 0\n",
    "        while True:\n",
    "            \n",
    "            end_idx = start_idx + blocksize\n",
    "            end_idx_cond = end_idx <= theta.shape[0]\n",
    "            \n",
    "            # remove blocks of observations\n",
    "\n",
    "            vars_jk = []\n",
    "\n",
    "            for var in [theta, S, r, u]:\n",
    "\n",
    "                var_jk = delete_obs_jk(var, start_idx, end_idx,\n",
    "                                       end_idx_cond)\n",
    "                vars_jk.append(var_jk)\n",
    "            \n",
    "            if start_idx < theta.shape[0]:\n",
    "                # Get our estimate\n",
    "                output_matrix, _ = self.solve(theta = vars_jk[0],\n",
    "                                              S = vars_jk[1],\n",
    "                                              r = vars_jk[2],\n",
    "                                              u = vars_jk[3],\n",
    "                                              printout = False,\n",
    "                                              est_init = self.est_init)\n",
    "\n",
    "                estimates_jk.append(output_matrix)\n",
    "\n",
    "                start_idx += blocksize\n",
    "            else:\n",
    "                break\n",
    "            \n",
    "        estimates_jk = np.array(estimates_jk)\n",
    "        full_est = self.output_matrix\n",
    "        \n",
    "        # calculate pseudo-values\n",
    "        n_blocks = int(nobs/blocksize)\n",
    "        pseudovalues = n_blocks * full_est - (n_blocks - 1) * estimates_jk\n",
    "        \n",
    "        # calculate jackknife se\n",
    "        pseudovalues = pseudovalues.reshape((n_blocks, theta.shape[1] * theta.shape[1]))\n",
    "        jknife_cov = np.cov(pseudovalues.T, ddof=1) / n_blocks\n",
    "        jknife_var = np.diag(jknife_cov)\n",
    "        jknife_se = np.sqrt(jknife_var)\n",
    "    \n",
    "        jknife_se  = jknife_se.reshape((theta.shape[1], theta.shape[1]))\n",
    "        \n",
    "        return jknife_se  \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
