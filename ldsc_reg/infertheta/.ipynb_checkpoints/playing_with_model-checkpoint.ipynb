{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'ldsc_72' from 'C:\\\\Users\\\\Hariharan\\\\Documents\\\\git_repos\\\\SNIPar\\\\ldsc_reg\\\\infertheta\\\\ldsc_72.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import ldsc_72 as ld\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from numba import jit, njit, prange, vectorize\n",
    "from helperfuncs import *\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# reloading modules\n",
    "importlib.reload(ld)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning there is no value for theta. Maybe consider simulating it\n",
      "No value for U given. Generating a vector of ones (all SNPs weighted equally)\n",
      "No value for r given. Generating a vector of ones for r\n",
      "Simulated LD scores!\n",
      "Effect Vectors Simulated!\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "N = int(100)\n",
    "S_size = int(N/2)\n",
    "S = np.array([np.array([[.5, 0], [0, .8]]),\n",
    "    np.array([[0.5, 0], [0, 0.8]])] * S_size )\n",
    "V = np.identity(2) * 0.5\n",
    "f = np.random.uniform(0, 1, N)\n",
    "\n",
    "\n",
    "\n",
    "model = ld.sibreg(S = S, f = f)\n",
    "model.simdata(V, N, simr = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No initial guess provided.\n",
      "Making 'optimal' matrix\n",
      "=================================================\n",
      "[[ 0.26164207 -4.67047071]\n",
      " [-4.67047071 42.54352214]]\n",
      "      fun: -529.4063339249765\n",
      " hess_inv: <3x3 LbfgsInvHessProduct with dtype=float64>\n",
      "      jac: array([-14592485.50491885,  -1587275.35175443,   -172652.98886016])\n",
      "  message: b'ABNORMAL_TERMINATION_IN_LNSRCH'\n",
      "     nfev: 78\n",
      "      nit: 4\n",
      "   status: 2\n",
      "  success: False\n",
      "        x: array([ 0.26164207, -4.67047071, 42.54352214])\n"
     ]
    }
   ],
   "source": [
    "output_matrix, result = model.solve()\n",
    "print(output_matrix)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A curious point here. If I simulate the data with ```r```, I HAVE to put the initial estimate as a zero matrix for it to converge. With the optimal matrix it doesn't converge. This is true only if the true ```V``` vector is large (diagonal elements are 10.0 for example).\n",
    "\n",
    "This is not true if the diagonal elements are small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[30.02415084  9.95263919]\n",
      " [ 9.95263919  0.7399902 ]]\n"
     ]
    }
   ],
   "source": [
    "jkse = model.jackknife_se()\n",
    "print(jkse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running LD Score Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chi sqaure stat for the direct effect\n",
    "chisq = N * model.theta[:, 0] ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "cons = np.ones(N)\n",
    "reg1 = sm.OLS(endog=chisq, exog=np.array([cons, model.r]).T, \n",
    "              missing='drop',\n",
    "              hasconst = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS Regression Results                                \n",
      "=======================================================================================\n",
      "Dep. Variable:                      y   R-squared (uncentered):                   0.395\n",
      "Model:                            OLS   Adj. R-squared (uncentered):              0.383\n",
      "Method:                 Least Squares   F-statistic:                              32.05\n",
      "Date:                Mon, 24 Aug 2020   Prob (F-statistic):                    1.95e-11\n",
      "Time:                        21:43:05   Log-Likelihood:                         -780.96\n",
      "No. Observations:                 100   AIC:                                      1566.\n",
      "Df Residuals:                      98   BIC:                                      1571.\n",
      "Df Model:                           2                                                  \n",
      "Covariance Type:            nonrobust                                                  \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const        162.1734    194.032      0.836      0.405    -222.876     547.223\n",
      "x1           102.9248     61.365      1.677      0.097     -18.852     224.702\n",
      "==============================================================================\n",
      "Omnibus:                       56.561   Durbin-Watson:                   1.546\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              186.928\n",
      "Skew:                           2.016   Prob(JB):                     2.57e-41\n",
      "Kurtosis:                       8.348   Cond. No.                         11.1\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "results = reg1.fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating LD score data which LDSC can read\n",
    "\n",
    "# main ldscore data\n",
    "ldscores = pd.DataFrame({'CHR' : np.array([22] * N),\n",
    "                        'SNP' : model.snp,\n",
    "                        'BP' : model.pos,\n",
    "                        'L2' : model.r})\n",
    "ldscores.to_csv(\"ldscores/22.l2.ldscore.gz\",\n",
    "                compression = 'gzip',\n",
    "                sep = \" \")\n",
    "\n",
    "# N data\n",
    "with open(\"ldscores/22.l2.M\", \"w\") as f:\n",
    "    f.write(str(N))\n",
    "\n",
    "with open(\"ldscores/22.l2.M_5_50\", \"w\") as f:\n",
    "    f.write(str(N))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading in Data simulated from Actual SNPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No value for U given. Generating a vector of ones (all SNPs weighted equally)\n",
      "No value for r given. Generating a vector of ones for r\n"
     ]
    }
   ],
   "source": [
    "# reading in causal data\n",
    "file = \"C:/Users/Hariharan/Documents/genoecon_work/snipardata/causal.hdf5\"\n",
    "\n",
    "hf = h5py.File(file, 'r')\n",
    "theta  = hf.get('estimate')[()]\n",
    "S = hf.get('estimate_covariance')[()]\n",
    "f = hf.get('freqs')[()]\n",
    "\n",
    "model = ld2.sibreg(S = S, f= f, theta = theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.001001e-10, 0.000000e+00, 0.000000e+00],\n",
       "        [0.000000e+00, 1.001001e-10, 0.000000e+00],\n",
       "        [0.000000e+00, 0.000000e+00, 1.001001e-10]]),\n",
       "       fun: -89275.12272475495\n",
       "  hess_inv: <6x6 LbfgsInvHessProduct with dtype=float64>\n",
       "       jac: array([-2.13456477e+08, -3.98672166e+07, -3.85922911e+07, -3.79255614e+08,\n",
       "        -3.12722864e+08, -3.26042514e+08])\n",
       "   message: b'ABNORMAL_TERMINATION_IN_LNSRCH'\n",
       "      nfev: 21\n",
       "       nit: 0\n",
       "    status: 2\n",
       "   success: False\n",
       "         x: array([1.e-06, 0.e+00, 0.e+00, 1.e-06, 0.e+00, 1.e-06]))"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.solve(est_init = np.zeros((3, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def core_logll_loop(V, N, S, theta, u, r):\n",
    "    \n",
    "    Gvec = np.zeros_like(V)\n",
    "    log_ll = 0\n",
    "        \n",
    "    for i in prange(N):\n",
    "\n",
    "\n",
    "        Si = S[i]\n",
    "        thetai = theta[i, :]\n",
    "        ui = u[i]\n",
    "        ri = r[i]\n",
    "        \n",
    "\n",
    "        d, ddash = Si.shape\n",
    "        assert d == ddash # Each S has to be a square matrix\n",
    "\n",
    "        # calculate log likelihood\n",
    "        log_ll += -(d/2) * np.log(2 * np.pi)\n",
    "        dit_sv = np.linalg.det(Si + ri * V)\n",
    "        log_ll += -(1/2) * np.log(dit_sv)\n",
    "        log_ll += -(1/2) * np.trace(np.outer(thetai, thetai) @ np.linalg.inv(Si + ri * V))\n",
    "        log_ll *= 1/ui\n",
    "\n",
    "        # calculate gradient\n",
    "        SV_inv = np.linalg.inv(Si + ri * V)\n",
    "        G = -(1 / 2) * SV_inv\n",
    "        G += (1 / 2) * np.dot(SV_inv,np.dot(np.outer(thetai, thetai),SV_inv))\n",
    "        G *= 1/ui\n",
    "\n",
    "        Gvec += G\n",
    "        \n",
    "    return log_ll, Gvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def numba_core_logll_loop(V, N, S, theta, u, r):\n",
    "    \n",
    "    Gvec = np.zeros_like(V)\n",
    "    log_ll = 0\n",
    "        \n",
    "    for i in prange(N):\n",
    "\n",
    "\n",
    "        Si = S[i]\n",
    "        thetai = theta[i, :]\n",
    "        ui = u[i]\n",
    "        ri = r[i]\n",
    "        \n",
    "\n",
    "        d, ddash = Si.shape\n",
    "        assert d == ddash # Each S has to be a square matrix\n",
    "\n",
    "        # calculate log likelihood\n",
    "        log_ll += -(d/2) * np.log(2 * np.pi)\n",
    "        dit_sv = np.linalg.det(Si + ri * V)\n",
    "        log_ll += -(1/2) * np.log(dit_sv)\n",
    "        log_ll += -(1/2) * np.trace(np.outer(thetai, thetai) @ np.linalg.inv(Si + ri * V))\n",
    "        log_ll *= 1/ui\n",
    "\n",
    "        # calculate gradient\n",
    "        SV_inv = np.linalg.inv(Si + ri * V)\n",
    "        G = -(1 / 2) * SV_inv\n",
    "        G += (1 / 2) * np.dot(SV_inv,np.dot(np.outer(thetai, thetai),SV_inv))\n",
    "        G *= 1/ui\n",
    "\n",
    "        Gvec += G\n",
    "        \n",
    "    return log_ll, Gvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outer_neg_logll_grad(V, theta, S, u, r, loopfunc):\n",
    "        \n",
    "    # ============================================ #\n",
    "    # returns negative log likelihood and negative\n",
    "    # of the gradient\n",
    "    # ============================================ #\n",
    "    \n",
    "    # Unflatten V into a matrix\n",
    "    d = S[0].shape[0]\n",
    "    V = return_to_symmetric(V, d)\n",
    "    \n",
    "    N = len(S)\n",
    "\n",
    "    log_ll, Gvec = loopfunc(V, N, S, theta, u, r)\n",
    "\n",
    "\n",
    "    Gvec = extract_upper_triangle(Gvec)\n",
    "\n",
    "    return -log_ll, -Gvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vin = extract_upper_triangle(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.1 ms ± 188 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "outer_neg_logll_grad(Vin, model.theta, model.S, model.u, model.r, \n",
    "                     loopfunc = core_logll_loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "801 µs ± 35.4 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "outer_neg_logll_grad(Vin, model.theta, model.S, model.u, model.r,\n",
    "                     loopfunc = numba_core_logll_loop)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
